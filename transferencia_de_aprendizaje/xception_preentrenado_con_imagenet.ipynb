{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoBRdgz/inteligencia_artificial/blob/main/transferencia_de_aprendizaje/xception_preentrenado_con_imagenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpfbqz00t18B"
      },
      "source": [
        "# Transferencia de aprendizaje (Transfer learning) y ajuste fino (fine-tuning)\n",
        "\n",
        "**Objetivo:** Se busca proveer de una guía completa en el aprendizaje por transferencia y ajuste fino, mejor conocido como *transfer learning & fine-tuning* respectivamente tomando como ejemplo la arquitectura Xception."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrqxeK_jt18I"
      },
      "source": [
        "## Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-vY-8mft18J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ_57w5ft18K"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "**Transferir aprendizaje** consiste en tomar características aprendidas en un problema y\n",
        "aprovechándolos en un nuevo problema similar. Por ejemplo, las características de un modelo que tiene\n",
        "aprendido a identificar mapaches puede ser útil para poner en marcha un modelo destinado a identificar\n",
        "  tanukis.\n",
        "\n",
        "El aprendizaje de transferencia generalmente se realiza para tareas en las que su conjunto de datos tiene muy pocos datos para\n",
        "  entrenar un modelo a escala real desde cero.\n",
        "\n",
        "La encarnación más común del aprendizaje por transferencia en el contexto del aprendizaje profundo es el\n",
        "  siguiente flujo de trabajo:\n",
        "\n",
        "1. Tome capas de un modelo previamente entrenado.\n",
        "2. Congelarlos, para evitar destruir la información que contienen durante\n",
        "  futuras rondas de entrenamiento.\n",
        "3. Agregue algunas capas nuevas que se puedan entrenar encima de las capas congeladas. Aprenderán a girar\n",
        "  las características antiguas en predicciones sobre un nuevo conjunto de datos.\n",
        "4. Entrene las nuevas capas en su conjunto de datos.\n",
        "\n",
        "Un último paso opcional es el **ajuste fino**, que consiste en descongelar todo el\n",
        "modelo que obtuvo arriba (o parte de él), y volver a entrenarlo en los nuevos datos con un\n",
        "Tasa de aprendizaje muy baja. Esto puede potencialmente lograr mejoras significativas, al\n",
        "  adaptando gradualmente las funciones preentrenadas a los nuevos datos.\n",
        "\n",
        "Primero, repasaremos en detalle la API `entrenable` de Keras, que subyace a la mayoría\n",
        "  transferir el aprendizaje y ajustar los flujos de trabajo.\n",
        "\n",
        "Luego, demostraremos el flujo de trabajo típico tomando un modelo previamente entrenado en el\n",
        "Conjunto de datos de ImageNet y volver a entrenarlo en la clasificación de Kaggle \"gatos contra perros\"\n",
        "  conjunto de datos\n",
        "\n",
        "Esto está adaptado de\n",
        "[Aprendizaje profundo con Python](https://www.manning.com/books/deep-learning-with-python)\n",
        "  y la publicación de blog de 2016\n",
        "[\"construir poderosos modelos de clasificación de imágenes usando muy poco\n",
        "  data\"](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CuHJnrHt18M"
      },
      "source": [
        "## Congelar capas: entender el atributo `entrenable`\n",
        "\n",
        "Las capas y los modelos tienen tres atributos de peso:\n",
        "\n",
        "- `pesos` es la lista de todas las variables de peso de la capa.\n",
        "- `trainable_weights` es la lista de aquellos que deben actualizarse (a través de gradiente\n",
        "  descenso) para minimizar la pérdida durante el entrenamiento.\n",
        "- `non_trainable_weights` es la lista de aquellos que no están destinados a ser entrenados.\n",
        "  Por lo general, el modelo los actualiza durante el pase hacia adelante.\n",
        "\n",
        "**Ejemplo: la capa `Densa` tiene 2 pesos entrenables (núcleo y sesgo)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REs-5QqMt18M"
      },
      "outputs": [],
      "source": [
        "layer = keras.layers.Dense(3)\n",
        "layer.build((None, 4))  # Create the weights\n",
        "\n",
        "print(\"weights:\", len(layer.weights))\n",
        "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
        "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE1q69AMt18N"
      },
      "source": [
        "En general, todos los pesos son pesos entrenables. La única capa integrada que tiene\n",
        "pesos no entrenables es la capa `BatchNormalization`. Utiliza pesos no entrenables.\n",
        "  para realizar un seguimiento de la media y la varianza de sus entradas durante el entrenamiento.\n",
        "Para aprender a usar pesos que no se pueden entrenar en sus propias capas personalizadas, consulte la\n",
        "[guía para escribir nuevas capas desde cero](https://keras.io/guides/making_new_layers_and_models_via_subclassing/).\n",
        "\n",
        "**Ejemplo: la capa `BatchNormalization` tiene 2 pesos entrenables y 2 no entrenables\n",
        "  pesos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeZUBw3nt18O"
      },
      "outputs": [],
      "source": [
        "layer = keras.layers.BatchNormalization()\n",
        "layer.build((None, 4))  # Create the weights\n",
        "\n",
        "print(\"weights:\", len(layer.weights))\n",
        "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
        "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_LZzryMt18O"
      },
      "source": [
        "Las capas y los modelos también cuentan con un atributo booleano `entrenable`. Su valor se puede cambiar.\n",
        "Establecer `layer.trainable` en `False` mueve todos los pesos de la capa de entrenable a\n",
        "no entrenable. Esto se llama \"congelar\" la capa: el estado de una capa congelada no\n",
        "actualizarse durante el entrenamiento (ya sea cuando se entrena con `fit()` o cuando se entrena con\n",
        "  cualquier bucle personalizado que dependa de `trainable_weights` para aplicar actualizaciones de gradiente).\n",
        "\n",
        "**Ejemplo: establecer `trainable` en `False`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzDtdaI8t18P"
      },
      "outputs": [],
      "source": [
        "layer = keras.layers.Dense(3)\n",
        "layer.build((None, 4))  # Create the weights\n",
        "layer.trainable = False  # Freeze the layer\n",
        "\n",
        "print(\"weights:\", len(layer.weights))\n",
        "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
        "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPQOj8vGt18Q"
      },
      "source": [
        "Cuando un peso entrenable se vuelve no entrenable, su valor ya no se actualiza durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7IemCTlt18Q"
      },
      "outputs": [],
      "source": [
        "# Make a model with 2 layers\n",
        "layer1 = keras.layers.Dense(3, activation=\"relu\")\n",
        "layer2 = keras.layers.Dense(3, activation=\"sigmoid\")\n",
        "model = keras.Sequential([keras.Input(shape=(3,)), layer1, layer2])\n",
        "\n",
        "# Freeze the first layer\n",
        "layer1.trainable = False\n",
        "\n",
        "# Keep a copy of the weights of layer1 for later reference\n",
        "initial_layer1_weights_values = layer1.get_weights()\n",
        "\n",
        "# Train the model\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
        "\n",
        "# Check that the weights of layer1 have not changed during training\n",
        "final_layer1_weights_values = layer1.get_weights()\n",
        "np.testing.assert_allclose(\n",
        "    initial_layer1_weights_values[0], final_layer1_weights_values[0]\n",
        ")\n",
        "np.testing.assert_allclose(\n",
        "    initial_layer1_weights_values[1], final_layer1_weights_values[1]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D32AoTxt18R"
      },
      "source": [
        "No confundir el atributo `layer.trainable` con el argumento `training` en `layer.__call__()` (que controla si la capa debe ejecutar su paso hacia adelante en modo de inferencia o modo de entrenamiento). Para obtener más información, consulte [Preguntas frecuentes de Keras](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axoBbmYNt18R"
      },
      "source": [
        "## Configuración recursiva del atributo `entrenable`\n",
        "\n",
        "Si establece `trainable = False` en un modelo o en cualquier capa que tenga subcapas,\n",
        "todas las capas de niños también se vuelven no entrenables.\n",
        "\n",
        "**Ejemplo:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYedgEsZt18R"
      },
      "outputs": [],
      "source": [
        "inner_model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(3,)),\n",
        "        keras.layers.Dense(3, activation=\"relu\"),\n",
        "        keras.layers.Dense(3, activation=\"relu\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [keras.Input(shape=(3,)), inner_model, keras.layers.Dense(3, activation=\"sigmoid\"),]\n",
        ")\n",
        "\n",
        "model.trainable = False  # Freeze the outer model\n",
        "\n",
        "assert inner_model.trainable == False  # All layers in `model` are now frozen\n",
        "assert inner_model.layers[0].trainable == False  # `trainable` is propagated recursively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lck8QStzt18S"
      },
      "source": [
        "## El típico flujo de trabajo de aprendizaje por transferencia\n",
        "\n",
        "Esto nos lleva a cómo se puede implementar un flujo de trabajo de aprendizaje de transferencia típico en Keras:\n",
        "\n",
        "1. Cree una instancia de un modelo base y cargue pesos previamente entrenados en él.\n",
        "2. Congele todas las capas en el modelo base configurando `trainable = False`.\n",
        "3. Cree un nuevo modelo encima de la salida de una (o varias) capas de la base\n",
        "  modelo.\n",
        "4. Entrene su nuevo modelo en su nuevo conjunto de datos.\n",
        "\n",
        "Tenga en cuenta que un flujo de trabajo alternativo y más ligero también podría ser:\n",
        "\n",
        "1. Cree una instancia de un modelo base y cargue pesos previamente entrenados en él.\n",
        "2. Ejecute su nuevo conjunto de datos y registre la salida de una (o varias) capas del modelo base. Esto se llama **extracción de características**.\n",
        "3. Use esa salida como datos de entrada para un modelo nuevo y más pequeño.\n",
        "\n",
        "Una ventaja clave de ese segundo flujo de trabajo es que solo ejecuta el modelo base una vez en\n",
        "  sus datos, en lugar de una vez por época de entrenamiento. Así que es mucho más rápido y más barato.\n",
        "\n",
        "Sin embargo, un problema con ese segundo flujo de trabajo es que no le permite dinámicamente\n",
        "modifique los datos de entrada de su nuevo modelo durante el entrenamiento, lo cual es necesario al hacer\n",
        "aumento de datos, por ejemplo. El aprendizaje por transferencia se usa típicamente para tareas cuando\n",
        "su nuevo conjunto de datos tiene muy pocos datos para entrenar un modelo a gran escala desde cero, y en\n",
        "tales escenarios el aumento de datos es muy importante. Así que en lo que sigue, nos centraremos\n",
        "  en el primer flujo de trabajo.\n",
        "\n",
        "Así es como se ve el primer flujo de trabajo en Keras:\n",
        "\n",
        "Primero, crea una instancia de un modelo base con pesos previamente entrenados.\n",
        "\n",
        "```python\n",
        "base_model = keras.applications.Xception(\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False)  # Do not include the ImageNet classifier at the top.\n",
        "```\n",
        "\n",
        "Luego, congela el modelo base.\n",
        "\n",
        "```python\n",
        "base_model.trainable = False\n",
        "```\n",
        "\n",
        "Crea un nuevo modelo.\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "# We make sure that the base_model is running in inference mode here,\n",
        "# by passing `training=False`. This is important for fine-tuning, as you will\n",
        "# learn in a few paragraphs.\n",
        "x = base_model(inputs, training=False)\n",
        "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "# A Dense classifier with a single unit (binary classification)\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "```\n",
        "\n",
        "Entrenar el modelo en datos nuevos.\n",
        "\n",
        "```python\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])\n",
        "model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXU_1yR-t18T"
      },
      "source": [
        "## Sintonia FINA\n",
        "\n",
        "Una vez que su modelo haya convergido en los nuevos datos, puede intentar descongelar todo o parte de\n",
        "  el modelo base y volver a entrenar todo el modelo de extremo a extremo con una tasa de aprendizaje muy baja.\n",
        "\n",
        "Este es un último paso opcional que potencialmente puede brindarle mejoras incrementales.\n",
        "  También podría conducir a un sobreajuste rápido, tenlo en cuenta.\n",
        "\n",
        "Es fundamental realizar este paso solo *después* de que el modelo con capas congeladas haya sido\n",
        "capacitados para la convergencia. Si mezcla capas entrenables inicializadas aleatoriamente con\n",
        "Capas entrenables que contienen funciones preentrenadas, las capas inicializadas aleatoriamente\n",
        "causar actualizaciones de gradiente muy grandes durante el entrenamiento, lo que destruirá su entrenamiento previo\n",
        "  características.\n",
        "\n",
        "También es fundamental usar una tasa de aprendizaje muy baja en esta etapa, porque\n",
        "está entrenando un modelo mucho más grande que en la primera ronda de entrenamiento, en un conjunto de datos\n",
        "  eso es típicamente muy pequeño.\n",
        "Como resultado, corre el riesgo de sobreajustarse muy rápidamente si aplica mucho peso.\n",
        "  actualizaciones Aquí, solo desea readaptar los pesos preentrenados de forma incremental.\n",
        "\n",
        "Así es como se implementa el ajuste fino de todo el modelo base:\n",
        "\n",
        "```python\n",
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# It's important to recompile your model after you make any changes\n",
        "# to the `trainable` attribute of any inner layer, so that your changes\n",
        "# are take into account\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
        "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "# Train end-to-end. Be careful to stop before you overfit!\n",
        "model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n",
        "```\n",
        "\n",
        "**Nota importante sobre `compile()` y `entrenable`**\n",
        "\n",
        "Llamar a `compile()` en un modelo está destinado a \"congelar\" el comportamiento de ese modelo. Esto\n",
        "  implica que el `entrenable`\n",
        "Los valores de los atributos en el momento en que se compila el modelo deben conservarse durante todo el\n",
        "  toda la vida de ese modelo,\n",
        "hasta que `compile` sea llamado de nuevo. Por lo tanto, si cambia cualquier valor 'entrenable', asegúrese de\n",
        "  para llamar a `compile()` de nuevo en su\n",
        "modelo para que sus cambios sean tomados en cuenta.\n",
        "\n",
        "**Notas importantes sobre la capa `BatchNormalization`**\n",
        "\n",
        "Muchos modelos de imagen contienen capas `BatchNormalization`. Esa capa es un caso especial en\n",
        "  todos los conteos imaginables. Aquí hay algunas cosas a tener en cuenta.\n",
        "\n",
        "- `BatchNormalization` contiene 2 pesos no entrenables que se actualizan durante\n",
        "capacitación. Estas son las variables que siguen la media y la varianza de las entradas.\n",
        "- Cuando establece `bn_layer.trainable = False`, la capa `BatchNormalization`\n",
        "se ejecuta en modo de inferencia y no actualizará sus estadísticas de media y varianza. Esto no es\n",
        "el caso de otras capas en general, como\n",
        "[weight trainability & inference/training modes are two orthogonal concepts](\n",
        "  https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n",
        "\n",
        "Pero los dos están empatados en el caso de la capa `BatchNormalization`.\n",
        "- Cuando descongela un modelo que contiene capas `BatchNormalization` para hacer\n",
        "ajuste fino, debe mantener las capas `BatchNormalization` en modo de inferencia\n",
        "  pasando `training=False` al llamar al modelo base.\n",
        "De lo contrario, las actualizaciones aplicadas a los pesos no entrenables destruirán repentinamente\n",
        "lo que el modelo ha aprendido.\n",
        "\n",
        "Verá este patrón en acción en el ejemplo completo al final de esta guía."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlX-vn15t18U"
      },
      "source": [
        "## Transfiera el aprendizaje y ajuste con un ciclo de entrenamiento personalizado\n",
        "\n",
        "Si en lugar de `fit()`, está utilizando su propio ciclo de entrenamiento de bajo nivel, el flujo de trabajo\n",
        "permanece esencialmente igual. Debe tener cuidado de tener en cuenta solo la lista.\n",
        "  `model.trainable_weights` al aplicar actualizaciones de gradiente:\n",
        "\n",
        "```python\n",
        "# Create base model\n",
        "base_model = keras.applications.Xception(\n",
        "    weights='imagenet',\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False)\n",
        "# Freeze base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top.\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = keras.optimizers.Adam()\n",
        "\n",
        "# Iterate over the batches of a dataset.\n",
        "for inputs, targets in new_dataset:\n",
        "    # Open a GradientTape.\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass.\n",
        "        predictions = model(inputs)\n",
        "        # Compute the loss value for this batch.\n",
        "        loss_value = loss_fn(targets, predictions)\n",
        "\n",
        "    # Get gradients of loss wrt the *trainable* weights.\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "    # Update the weights of the model.\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHvtkVP1t18V"
      },
      "source": [
        "Del mismo modo para el ajuste fino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3oanVeCt18V"
      },
      "source": [
        "## Un ejemplo completo: ajuste fino de un modelo de clasificación de imágenes en un conjunto de datos de gatos contra perros\n",
        "\n",
        "Para solidificar estos conceptos, lo guiaremos a través de una transferencia concreta de un extremo a otro\n",
        "Ejemplo de aprendizaje y ajuste. Cargaremos el modelo Xception, pre-entrenado en\n",
        "  ImageNet y utilícelo en el conjunto de datos de clasificación \"gatos contra perros\" de Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syw9utqwt18W"
      },
      "source": [
        "### Obtener los datos\n",
        "\n",
        "Primero, obtengamos el conjunto de datos de gatos contra perros usando TFDS. Si tiene su propio conjunto de datos,\n",
        "probablemente querrás usar la utilidad\n",
        "`tf.keras.utils.image_dataset_from_directory` para generar etiquetas similares\n",
        "  objetos de conjunto de datos de un conjunto de imágenes en disco archivadas en carpetas específicas de clase.\n",
        "\n",
        "Transferir el aprendizaje es más útil cuando se trabaja con conjuntos de datos muy pequeños. para mantener nuestro\n",
        "conjunto de datos pequeño, usaremos el 40% de los datos de entrenamiento originales (25,000 imágenes) para\n",
        "  capacitación, 10% para validación y 10% para prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBtyNtzft18X"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "train_ds, validation_ds, test_ds = tfds.load(\n",
        "    \"cats_vs_dogs\",\n",
        "    # Reserve 10% for validation and 10% for test\n",
        "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
        "    as_supervised=True,  # Include labels\n",
        ")\n",
        "\n",
        "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
        "print(\n",
        "    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\n",
        ")\n",
        "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7agop57ut18Y"
      },
      "source": [
        "Estas son las primeras 9 imágenes en el conjunto de datos de entrenamiento; como puede ver, todas tienen diferentes tamaños."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ui6JN83t18Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, (image, label) in enumerate(train_ds.take(9)):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(int(label))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6fJ_tH4t18Z"
      },
      "source": [
        "También podemos ver que la etiqueta 1 es \"perro\" y la etiqueta 0 es \"gato\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAysjFDEt18Z"
      },
      "source": [
        "### Estandarizando los datos\n",
        "\n",
        "Nuestras imágenes en bruto tienen una variedad de tamaños. Además, cada píxel consta de 3 enteros\n",
        "valores entre 0 y 255 (valores de nivel RGB). Esto no es muy adecuado para alimentar a un\n",
        "  red neuronal Tenemos que hacer 2 cosas:\n",
        "\n",
        "- Estandarizar a un tamaño de imagen fijo. Elegimos 150x150.\n",
        "- Normalice los valores de píxel entre -1 y 1. Haremos esto usando una capa de `Normalización` como\n",
        "  parte del propio modelo.\n",
        "\n",
        "En general, es una buena práctica desarrollar modelos que toman datos sin procesar como entrada, como\n",
        "a diferencia de los modelos que toman datos ya preprocesados. La razón es que, si su\n",
        "el modelo espera datos preprocesados, cada vez que exporta su modelo para usarlo en otro lugar\n",
        "(en un navegador web, en una aplicación móvil), deberá volver a implementar exactamente el mismo\n",
        "tubería de preprocesamiento. Esto se vuelve muy complicado muy rápidamente. Así que deberíamos hacer lo mínimo\n",
        "  posible cantidad de preprocesamiento antes de llegar al modelo.\n",
        "\n",
        "Aquí, cambiaremos el tamaño de la imagen en la canalización de datos (porque una red neuronal profunda puede\n",
        "procesar solo lotes contiguos de datos), y haremos la escala del valor de entrada como parte\n",
        "  del modelo, cuando lo creamos.\n",
        "\n",
        "Redimensionemos las imágenes a 150x150:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uiXVN6qt18a"
      },
      "outputs": [],
      "source": [
        "size = (150, 150)\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1UMFP9st18a"
      },
      "source": [
        "Además, agrupemos los datos por lotes y utilicemos el almacenamiento en caché y la captación previa para optimizar la velocidad de carga."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDaSH58ot18b"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvM5c95Ot18b"
      },
      "source": [
        "### Usar aumento de datos aleatorios\n",
        "\n",
        "Cuando no tiene un conjunto de datos de imágenes grande, es una buena práctica introducir artificialmente diversidad de muestras aplicando transformaciones aleatorias pero realistas a las imágenes de entrenamiento, como cambios horizontales aleatorios o pequeñas rotaciones aleatorias. Esto ayuda a exponer el modelo a diferentes aspectos de los datos de entrenamiento mientras ralentiza el sobreajuste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awDOtujqt18c"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMvAzKWut18c"
      },
      "source": [
        "Visualicemos cómo se ve la primera imagen del primer lote después de varias transformaciones aleatorias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAtIG8mZt18d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for images, labels in train_ds.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = images[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(\n",
        "            tf.expand_dims(first_image, 0), training=True\n",
        "        )\n",
        "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
        "        plt.title(int(labels[0]))\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vson6ij4t18d"
      },
      "source": [
        "## Construir un modelo\n",
        "\n",
        "Ahora construyamos un modelo que siga el modelo que hemos explicado anteriormente.\n",
        "\n",
        "Tenga en cuenta que:\n",
        "\n",
        "- Agregamos una capa de `Rescaling` para escalar los valores de entrada (inicialmente en `[0, 255]`\n",
        "  rango) al rango `[-1, 1]`.\n",
        "- Agregamos una capa `Dropout` antes de la capa de clasificación, para la regularización.\n",
        "- Nos aseguramos de pasar `training=False` al llamar al modelo base, para que\n",
        "se ejecuta en modo de inferencia, por lo que las estadísticas de normas por lotes no se actualizan\n",
        "incluso después de descongelar el modelo base para realizar ajustes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCMaPeUNt18e"
      },
      "outputs": [],
      "source": [
        "base_model = keras.applications.Xception(\n",
        "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False,\n",
        ")  # Do not include the ImageNet classifier at the top.\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = data_augmentation(inputs)  # Apply random data augmentation\n",
        "\n",
        "# Pre-trained Xception weights requires that input be scaled\n",
        "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
        "# outputs: `(inputs * scale) + offset`\n",
        "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
        "x = scale_layer(x)\n",
        "\n",
        "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
        "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
        "# base_model is running in inference mode here.\n",
        "x = base_model(x, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W6MKuITt18e"
      },
      "source": [
        "## Entrenamiento de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8rHLs0kt18e"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo0KjwXKt18f"
      },
      "source": [
        "## Haga una ronda de ajuste fino de todo el modelo\n",
        "\n",
        "Finalmente, descongelemos el modelo base y entrenemos todo el modelo de principio a fin con una tasa de aprendizaje baja.\n",
        "\n",
        "Es importante destacar que, aunque el modelo base se vuelve entrenable, todavía se ejecuta en modo de inferencia ya que pasamos `training=False` cuando lo llamamos cuando construimos el modelo. Esto significa que las capas de normalización de lotes internas no actualizarán sus estadísticas de lotes. Si lo hicieran, causarían estragos en las representaciones aprendidas por el modelo hasta el momento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQedEO2zt18f"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
        "# since we passed `training=False` when calling it. This means that\n",
        "# the batchnorm layers will not update their batch statistics.\n",
        "# This prevents the batchnorm layers from undoing all the training\n",
        "# we've done so far.\n",
        "\n",
        "base_model.trainable = True\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjUHcwovt18f"
      },
      "source": [
        "Después de 10 épocas, el ajuste fino nos brinda una buena mejora aquí."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}