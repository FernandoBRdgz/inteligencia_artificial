{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMsCKf7ZDGThCjeaxBES4DP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoBRdgz/inteligencia_artificial/blob/main/incrustaciones_de_palabras/word2vec_yelp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducción\n",
        "\n",
        "El conjunto de datos de Yelp es un subconjunto de nuestros negocios, reseñas y datos de usuario para su uso con fines personales, educativos y académicos. Disponible como archivos JSON, úselo para enseñar a los estudiantes acerca de las bases de datos, para aprender NLP o para obtener datos de producción de muestra mientras aprende a crear aplicaciones móviles.\n",
        "\n",
        "Enlace al conjunto de datos: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset"
      ],
      "metadata": {
        "id": "da3gt9aJTR8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3aGrXdCRym3",
        "outputId": "54d3d1ce-4003-4c9b-dbdc-7c8a030bcaf0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "xjsnVrfIR-YD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "hzSyWvyaR_pr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory = os.path.join(main_path, 'data', 'yelp_dataset')"
      ],
      "metadata": {
        "id": "t4XtWhEHSgIj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "businesses_filepath = os.path.join(data_directory, 'yelp_academic_dataset_business.json')"
      ],
      "metadata": {
        "id": "1xPPxy6HShn3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkh5zqfTPbt0",
        "outputId": "fae4362b-735f-4349-d770-0880201ad4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('{\"business_id\":\"Pns2l4eNsfO8kk83dixA6A\",\"name\":\"Abby Rappoport, LAC, '\n",
            " 'CMQ\",\"address\":\"1616 Chapala St, Ste 2\",\"city\":\"Santa '\n",
            " 'Barbara\",\"state\":\"CA\",\"postal_code\":\"93101\",\"latitude\":34.4266787,\"longitude\":-119.7111968,\"stars\":5.0,\"review_count\":7,\"is_open\":0,\"attributes\":{\"ByAppointmentOnly\":\"True\"},\"categories\":\"Doctors, '\n",
            " 'Traditional Chinese Medicine, Naturopathic\\\\/Holistic, Acupuncture, Health & '\n",
            " 'Medical, Nutritionists\",\"hours\":null}\\n')\n"
          ]
        }
      ],
      "source": [
        "with open(businesses_filepath) as f:\n",
        "    first_business_record = f.readline() \n",
        "\n",
        "pprint(first_business_record)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_json_filepath = os.path.join(data_directory, 'yelp_academic_dataset_review.json')"
      ],
      "metadata": {
        "id": "0SPGh3uUVBac"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(review_json_filepath) as f:\n",
        "    first_review_record = f.readline()\n",
        "    \n",
        "pprint(first_review_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6pAhgvSSilq",
        "outputId": "7308be40-dbdf-4fa6-d37d-3c60e58b25e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('{\"review_id\":\"KU_O5udG6zpxOg-VcAEodg\",\"user_id\":\"mh_-eMZ6K5RLWhZyISBhwA\",\"business_id\":\"XQfwVwDr-v0ZS3_CbbE5Xw\",\"stars\":3.0,\"useful\":0,\"funny\":0,\"cool\":0,\"text\":\"If '\n",
            " 'you decide to eat here, just be aware it is going to take about 2 hours from '\n",
            " 'beginning to end. We have tried it multiple times, because I want to like '\n",
            " \"it! I have been to it's other locations in NJ and never had a bad \"\n",
            " 'experience. \\\\n\\\\nThe food is good, but it takes a very long time to come '\n",
            " 'out. The waitstaff is very young, but usually pleasant. We have just had too '\n",
            " 'many experiences where we spent way too long waiting. We usually opt for '\n",
            " 'another diner or restaurant on the weekends, in order to be done '\n",
            " 'quicker.\",\"date\":\"2018-07-07 22:09:11\"}\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "restaurant_ids = set()\n",
        "\n",
        "with open(businesses_filepath) as f:    \n",
        "    for business_json in f:\n",
        "        business = json.loads(business_json)\n",
        "        if not business.get('categories'):\n",
        "            continue\n",
        "        if 'Restaurants' not in business['categories']:\n",
        "            continue\n",
        "        restaurant_ids.add(business['business_id'])\n",
        "\n",
        "restaurant_ids = frozenset(restaurant_ids)\n",
        "\n",
        "pprint(f'{len(restaurant_ids):,} restaurants in the dataset.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BciTv5xFVFF2",
        "outputId": "406ae839-d992-4e93-ae2d-972f66086830"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'52,268 restaurants in the dataset.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scratch_directory = os.path.join(data_directory, 'scratch')\n",
        "\n",
        "try:\n",
        "    os.mkdir(scratch_directory)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "\n",
        "review_txt_filepath = os.path.join(scratch_directory, 'review_text_all.txt')"
      ],
      "metadata": {
        "id": "M2GnvlOAVbph"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    review_count = 0\n",
        "    with open(review_txt_filepath, 'w') as review_txt_file:\n",
        "        with open(review_json_filepath) as review_json_file:\n",
        "            for review_json in review_json_file:\n",
        "                review = json.loads(review_json)\n",
        "                if review['business_id'] not in restaurant_ids:\n",
        "                    continue\n",
        "                review_txt_file.write(review['text'].replace('\\n', '\\\\n') + '\\n')\n",
        "                review_count += 1\n",
        "    print(f'Text from {review_count:,} restaurant reviews written to the new txt file.')\n",
        "    \n",
        "else:\n",
        "    with open(review_txt_filepath) as review_txt_file:\n",
        "        for review_count, line in enumerate(review_txt_file):\n",
        "            pass\n",
        "        \n",
        "    print(f'Text from {review_count + 1:,} restaurant reviews in the txt file.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFC_H3xEVs_G",
        "outputId": "a962044a-5170-4b28-9f5f-940563b7b1e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text from 4,725,884 restaurant reviews in the txt file.\n",
            "CPU times: user 7.78 s, sys: 1.73 s, total: 9.51 s\n",
            "Wall time: 25.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import pandas as pd\n",
        "import itertools as it"
      ],
      "metadata": {
        "id": "9yYRUPI2052x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Pl02Eu1drE",
        "outputId": "a33db1dd-4413-484e-b1ba-0352a719ff32"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-11 19:45:11.631391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "id": "8dd5N-2E07rv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_num = 42\n",
        "\n",
        "with open(review_txt_filepath) as f:\n",
        "    sample_review = list(it.islice(f, review_num, review_num+1))[0]\n",
        "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
        "        \n",
        "print(sample_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEmslJt112ck",
        "outputId": "47025689-0719-4e0d-f761-766c9a9341ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excellent service! Great diner food and breakfast is served all day. Came here for lunch- they were busy but very friendly and hospitable. Easy to get to off the 295.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "parsed_review = nlp(sample_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxuOqRWc2Tn7",
        "outputId": "d317ddeb-849d-48c6-afe6-bcbe7c026624"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 18.6 ms, sys: 4.79 ms, total: 23.4 ms\n",
            "Wall time: 34.7 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(parsed_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9bBtStv2ZLk",
        "outputId": "ea55d969-1693-457f-c8b7-1364b0e89085"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excellent service! Great diner food and breakfast is served all day. Came here for lunch- they were busy but very friendly and hospitable. Easy to get to off the 295.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(parsed_review, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "l3DJVlZg2lMm",
        "outputId": "6c332136-9eed-4595-c6f8-1524ba7f7bc8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Excellent service! Great diner food and breakfast is served \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    all day\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". Came here for lunch- they were busy but very friendly and hospitable. Easy to get to off the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    295\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              ".</br></div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models.word2vec import LineSentence"
      ],
      "metadata": {
        "id": "8Ggq42Ch6BnG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def punct_space(token):\n",
        "    return token.is_punct or token.is_space\n",
        "\n",
        "def pronoun_lemmatize(token):\n",
        "    if token.lemma_ == '-PRON-':\n",
        "        return token.lower_\n",
        "    \n",
        "    else:\n",
        "        return token.lemma_.lower()\n",
        "\n",
        "def line_review(filename):\n",
        "    with open(filename) as f:\n",
        "        for review in f:\n",
        "            yield review.replace('\\\\n', '\\n')"
      ],
      "metadata": {
        "id": "fchScF996U-j"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_lemmatized_filepath = os.path.join(scratch_directory, 'review_lemmatized_all.txt')\n",
        "sentence_lemmatized_filepath = os.path.join(scratch_directory, 'sentence_lemmatized_all.txt')"
      ],
      "metadata": {
        "id": "BWmDFwDs6ovk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    with open(review_lemmatized_filepath, 'w') as review_file:\n",
        "        with open(sentence_lemmatized_filepath, 'w') as sentence_file:\n",
        "            pipe = nlp.pipe(\n",
        "                line_review(review_txt_filepath),\n",
        "                batch_size=5000\n",
        "                )\n",
        "            \n",
        "            for parsed_review in pipe:\n",
        "                lemmatized_review = ' '.join([\n",
        "                    pronoun_lemmatize(token)\n",
        "                    for token in parsed_review\n",
        "                    if not punct_space(token)\n",
        "                    ])\n",
        "                \n",
        "                review_file.write(lemmatized_review + '\\n')\n",
        "        \n",
        "                for sent in parsed_review.sents:\n",
        "                    lemmatized_sentence = ' '.join([\n",
        "                        pronoun_lemmatize(token)\n",
        "                        for token in sent\n",
        "                        if not punct_space(token)\n",
        "                        ])\n",
        "                    \n",
        "                    sentence_file.write(lemmatized_sentence + '\\n')"
      ],
      "metadata": {
        "id": "0Vrh0J-M6vc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570f2113-265d-472b-aa4d-53ebf830ed21"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
            "Wall time: 10.3 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_unigrams = LineSentence(sentence_lemmatized_filepath)"
      ],
      "metadata": {
        "id": "2q7wDjjcgE4Z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence_unigrams in it.islice(sentences_unigrams, 60, 70):\n",
        "    print(' '.join(sentence_unigrams))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07D4jzZQgHM3",
        "outputId": "459b787e-1215-4950-e085-47b92e07c52c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bun make the sonoran dog\n",
            "\n",
            "it be like a snuggie for the pup\n",
            "\n",
            "a first it seem ridiculous and almost like it be go to be too much exactly like everyone 's favorite blanket with sleeve\n",
            "\n",
            "too much softness too much smush too indulgent\n",
            "\n",
            "wrong\n",
            "\n",
            "it be warm soft chewy fragrant and it succeed where other famed sonoran dogs fail\n",
            "\n",
            "the hot dog itself be flavorful but i would prefer that it or the bacon have a little more bite or snap to well hold their own against the dominant mustard and onion\n",
            "\n",
            "i be with the masse on the carne asada caramelo\n",
            "\n",
            "excellent tortilla salty melty cheese and great carne\n",
            "\n",
            "super cheap and you can drive through\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model_filepath = os.path.join(scratch_directory, 'bigram_phrase_model')"
      ],
      "metadata": {
        "id": "8CYl_hdwWPD7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "\n",
        "    bigram_phrases = Phrases(sentences_unigrams)\n",
        "    bigram_phrases = Phraser(bigram_phrases)\n",
        "    bigram_phrases.save(bigram_model_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0k_JSmfWUdi",
        "outputId": "404946ab-6e2c-424f-a2e0-48209c0815f9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 22s, sys: 763 ms, total: 1min 23s\n",
            "Wall time: 1min 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_phrases = Phraser.load(bigram_model_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD_23AzBWgAM",
        "outputId": "ae71006a-f248-436f-a7b5-2796f7aca4cb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.phrases:older version of FrozenPhrases loaded without corpus_word_count\n",
            "WARNING:gensim.models.phrases:setting corpus_word_count to 0, do not use it in your scoring function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_bigrams_filepath = os.path.join(scratch_directory, 'sentence_bigram_phrases_all.txt')"
      ],
      "metadata": {
        "id": "HxMnQIX8XTwq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "if execute:\n",
        "    with open(sentences_bigrams_filepath, 'w') as f:\n",
        "        for sentence_unigrams in sentences_unigrams:\n",
        "            sentence_bigrams = ' '.join(bigram_phrases[sentence_unigrams])\n",
        "            f.write(sentence_bigrams + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQBJJsK_XTfz",
        "outputId": "245de25d-6666-47ca-ee7d-47eadaba9adf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 55.6 s, sys: 679 ms, total: 56.3 s\n",
            "Wall time: 59.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_bigrams = LineSentence(sentences_bigrams_filepath)"
      ],
      "metadata": {
        "id": "tWlC56gIXhb4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence_bigrams in it.islice(sentences_bigrams, 60, 70):\n",
        "    print(' '.join(sentence_bigrams))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsgvZpPbZanO",
        "outputId": "404f20c1-4a10-40d1-b3d1-1a9bdaed27ef"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bun make the sonoran_dog\n",
            "\n",
            "it be like a snuggie for the pup\n",
            "\n",
            "a first it seem ridiculous and almost like it be go to be too much exactly like everyone 's favorite blanket with sleeve\n",
            "\n",
            "too much softness too much smush too indulgent\n",
            "\n",
            "wrong\n",
            "\n",
            "it be warm soft chewy fragrant and it succeed where other famed sonoran_dogs fail\n",
            "\n",
            "the hot_dog itself be flavorful but i would prefer that it or the bacon have a little more bite or snap to well hold their own against the dominant mustard and onion\n",
            "\n",
            "i be with the masse on the carne_asada caramelo\n",
            "\n",
            "excellent tortilla salty melty_cheese and great carne\n",
            "\n",
            "super cheap and you can drive_through\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_model_filepath = os.path.join(scratch_directory, 'trigram_phrase_model')"
      ],
      "metadata": {
        "id": "DUM4HdVFZhf5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "\n",
        "    trigram_phrases = Phrases(sentences_bigrams)\n",
        "    trigram_phrases = Phraser(trigram_phrases)\n",
        "    trigram_phrases.save(trigram_model_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4FaYmNjZj_T",
        "outputId": "3ee4e806-93a6-46e7-caf7-b729fc534c30"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 26s, sys: 1 s, total: 1min 27s\n",
            "Wall time: 1min 30s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_phrases = Phraser.load(trigram_model_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW-EMxpYZpxd",
        "outputId": "62e418b9-3677-4065-8a1f-75f59c49a516"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.phrases:older version of FrozenPhrases loaded without corpus_word_count\n",
            "WARNING:gensim.models.phrases:setting corpus_word_count to 0, do not use it in your scoring function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_trigrams_filepath = os.path.join(scratch_directory, 'sentence_trigram_phrases_all.txt')"
      ],
      "metadata": {
        "id": "RTdM18RkZpt5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    with open(sentences_trigrams_filepath, 'w') as f:\n",
        "        for sentence_bigrams in sentences_bigrams:\n",
        "            sentence_trigrams = ' '.join(trigram_phrases[sentence_bigrams])\n",
        "            f.write(sentence_trigrams + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEVC1Xt9Z6Oo",
        "outputId": "e3d52141-c906-4483-9510-204f5c58fdef"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 58 s, sys: 676 ms, total: 58.7 s\n",
            "Wall time: 1min 3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_trigrams = LineSentence(sentences_trigrams_filepath)"
      ],
      "metadata": {
        "id": "LCk86gtvZ58O"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence_trigrams in it.islice(sentences_trigrams, 60, 70):\n",
        "    print(' '.join(sentence_trigrams))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPDwwRS8aD2v",
        "outputId": "907ff2d4-2a4b-4b2d-8721-3f73350f6d82"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bun make the sonoran_dog\n",
            "\n",
            "it be like a snuggie for the pup\n",
            "\n",
            "a first it seem ridiculous and almost like it be go to be too much exactly like everyone 's favorite blanket with sleeve\n",
            "\n",
            "too much softness too much smush too indulgent\n",
            "\n",
            "wrong\n",
            "\n",
            "it be warm soft chewy fragrant and it succeed where other famed sonoran_dogs fail\n",
            "\n",
            "the hot_dog itself be flavorful but i would prefer that it or the bacon have a little more bite or snap to well hold their own against the dominant mustard and onion\n",
            "\n",
            "i be with the masse on the carne_asada_caramelo\n",
            "\n",
            "excellent tortilla salty melty_cheese and great carne\n",
            "\n",
            "super cheap and you can drive_through\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_trigrams_filepath = os.path.join(scratch_directory, 'review_trigrams_all.txt')"
      ],
      "metadata": {
        "id": "SUVhg9pkaDmX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    reviews_lemmatized = LineSentence(review_lemmatized_filepath)\n",
        "\n",
        "    with open(review_trigrams_filepath, 'w') as f:\n",
        "        \n",
        "        for review_unigrams in reviews_lemmatized:\n",
        "            review_bigrams = bigram_phrases[review_unigrams]\n",
        "            review_trigrams = trigram_phrases[review_bigrams]\n",
        "\n",
        "            review_trigrams = [\n",
        "                term\n",
        "                for term in review_trigrams\n",
        "                if term not in nlp.Defaults.stop_words\n",
        "                ]\n",
        "\n",
        "            review_trigrams = ' '.join(review_trigrams)\n",
        "            f.write(review_trigrams + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2j6KA-OaHkr",
        "outputId": "686c3e70-bedd-4064-ba6d-e67ed593ed67"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 39s, sys: 707 ms, total: 1min 40s\n",
            "Wall time: 1min 45s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_num = 0\n",
        "\n",
        "print('Original:' + '\\n')\n",
        "\n",
        "for review in it.islice(line_review(review_txt_filepath), review_num, review_num+1):\n",
        "    print(review)\n",
        "\n",
        "print('----' + '\\n')\n",
        "print('Transformed:' + '\\n')\n",
        "\n",
        "with open(review_trigrams_filepath) as f:\n",
        "    for review in it.islice(f, review_num, review_num+1):\n",
        "        print(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CPrb4iAaMrc",
        "outputId": "981dd8b6-0c82-4115-ca7b-065785394695"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            "\n",
            "If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \n",
            "\n",
            "The food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\n",
            "\n",
            "----\n",
            "\n",
            "Transformed:\n",
            "\n",
            "decide eat aware 2 hour begin end try multiple time want like location nj bad experience food good long time come waitstaff young usually pleasant experience spend way long wait usually opt diner restaurant weekend order quick\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Por hacer**\n",
        "\n",
        "* Añadir comentarios\n",
        "* Incrustaciones de palabra con Word2vec\n",
        "* Visualizaciones\n",
        "* Modelación de tópicos con LDA\n",
        "* Álgebra de palabras"
      ],
      "metadata": {
        "id": "pq0aqfH1YqAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referencias**\n",
        "\n",
        "* https://spacy.io/\n",
        "* https://radimrehurek.com/gensim/\n",
        "* https://github.com/pwharrison/modern-nlp-in-python-2019/blob/master/notebooks/Modern_NLP_in_Python.ipynb"
      ],
      "metadata": {
        "id": "QClEkJKWaKpp"
      }
    }
  ]
}