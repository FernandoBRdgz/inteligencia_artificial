{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3lBeeOvDzHVRUI0A3m936",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoBRdgz/inteligencia_artificial/blob/main/incrustaciones_de_palabras/word2vec_yelp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducción\n",
        "\n",
        "El conjunto de datos de Yelp es un subconjunto de nuestros negocios, reseñas y datos de usuario para su uso con fines personales, educativos y académicos. Disponible como archivos JSON, úselo para enseñar a los estudiantes acerca de las bases de datos, para aprender NLP o para obtener datos de producción de muestra mientras aprende a crear aplicaciones móviles.\n",
        "\n",
        "Enlace al conjunto de datos: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset"
      ],
      "metadata": {
        "id": "da3gt9aJTR8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L3aGrXdCRym3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "xjsnVrfIR-YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "hzSyWvyaR_pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory = os.path.join(main_path, 'data', 'yelp_dataset')"
      ],
      "metadata": {
        "id": "t4XtWhEHSgIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "businesses_filepath = os.path.join(data_directory, 'yelp_academic_dataset_business.json')"
      ],
      "metadata": {
        "id": "1xPPxy6HShn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkh5zqfTPbt0"
      },
      "outputs": [],
      "source": [
        "with open(businesses_filepath) as f:\n",
        "    first_business_record = f.readline() \n",
        "\n",
        "pprint(first_business_record)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_json_filepath = os.path.join(data_directory, 'yelp_academic_dataset_review.json')"
      ],
      "metadata": {
        "id": "0SPGh3uUVBac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(review_json_filepath) as f:\n",
        "    first_review_record = f.readline()\n",
        "    \n",
        "pprint(first_review_record)"
      ],
      "metadata": {
        "id": "s6pAhgvSSilq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restaurant_ids = set()\n",
        "\n",
        "with open(businesses_filepath) as f:    \n",
        "    for business_json in f:\n",
        "        business = json.loads(business_json)\n",
        "        if not business.get('categories'):\n",
        "            continue\n",
        "        if 'Restaurants' not in business['categories']:\n",
        "            continue\n",
        "        restaurant_ids.add(business['business_id'])\n",
        "\n",
        "restaurant_ids = frozenset(restaurant_ids)\n",
        "\n",
        "pprint(f'{len(restaurant_ids):,} restaurants in the dataset.')"
      ],
      "metadata": {
        "id": "BciTv5xFVFF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scratch_directory = os.path.join(data_directory, 'scratch')\n",
        "\n",
        "try:\n",
        "    os.mkdir(scratch_directory)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "\n",
        "review_txt_filepath = os.path.join(scratch_directory, 'review_text_all.txt')"
      ],
      "metadata": {
        "id": "M2GnvlOAVbph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    review_count = 0\n",
        "    with open(review_txt_filepath, 'w') as review_txt_file:\n",
        "        with open(review_json_filepath) as review_json_file:\n",
        "            for review_json in review_json_file:\n",
        "                review = json.loads(review_json)\n",
        "                if review['business_id'] not in restaurant_ids:\n",
        "                    continue\n",
        "                review_txt_file.write(review['text'].replace('\\n', '\\\\n') + '\\n')\n",
        "                review_count += 1\n",
        "    print(f'Text from {review_count:,} restaurant reviews written to the new txt file.')\n",
        "    \n",
        "else:\n",
        "    with open(review_txt_filepath) as review_txt_file:\n",
        "        for review_count, line in enumerate(review_txt_file):\n",
        "            pass\n",
        "        \n",
        "    print(f'Text from {review_count + 1:,} restaurant reviews in the txt file.')"
      ],
      "metadata": {
        "id": "MFC_H3xEVs_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import pandas as pd\n",
        "import itertools as it"
      ],
      "metadata": {
        "id": "9yYRUPI2052x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "C7Pl02Eu1drE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "id": "8dd5N-2E07rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_num = 42\n",
        "\n",
        "with open(review_txt_filepath) as f:\n",
        "    sample_review = list(it.islice(f, review_num, review_num+1))[0]\n",
        "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
        "        \n",
        "print(sample_review)"
      ],
      "metadata": {
        "id": "OEmslJt112ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "parsed_review = nlp(sample_review)"
      ],
      "metadata": {
        "id": "nxuOqRWc2Tn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(parsed_review)"
      ],
      "metadata": {
        "id": "o9bBtStv2ZLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(parsed_review, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "l3DJVlZg2lMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models.word2vec import LineSentence"
      ],
      "metadata": {
        "id": "8Ggq42Ch6BnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def punct_space(token):\n",
        "    return token.is_punct or token.is_space\n",
        "\n",
        "def pronoun_lemmatize(token):\n",
        "    if token.lemma_ == '-PRON-':\n",
        "        return token.lower_\n",
        "    \n",
        "    else:\n",
        "        return token.lemma_.lower()\n",
        "\n",
        "def line_review(filename):\n",
        "    with open(filename) as f:\n",
        "        for review in f:\n",
        "            yield review.replace('\\\\n', '\\n')"
      ],
      "metadata": {
        "id": "fchScF996U-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_lemmatized_filepath = os.path.join(scratch_directory, 'review_lemmatized_all.txt')\n",
        "sentence_lemmatized_filepath = os.path.join(scratch_directory, 'sentence_lemmatized_all.txt')"
      ],
      "metadata": {
        "id": "BWmDFwDs6ovk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    with open(review_lemmatized_filepath, 'w') as review_file:\n",
        "        with open(sentence_lemmatized_filepath, 'w') as sentence_file:\n",
        "            pipe = nlp.pipe(\n",
        "                line_review(review_txt_filepath),\n",
        "                batch_size=5000\n",
        "                )\n",
        "            \n",
        "            for parsed_review in pipe:\n",
        "                lemmatized_review = ' '.join([\n",
        "                    pronoun_lemmatize(token)\n",
        "                    for token in parsed_review\n",
        "                    if not punct_space(token)\n",
        "                    ])\n",
        "                \n",
        "                review_file.write(lemmatized_review + '\\n')\n",
        "        \n",
        "                for sent in parsed_review.sents:\n",
        "                    lemmatized_sentence = ' '.join([\n",
        "                        pronoun_lemmatize(token)\n",
        "                        for token in sent\n",
        "                        if not punct_space(token)\n",
        "                        ])\n",
        "                    \n",
        "                    sentence_file.write(lemmatized_sentence + '\\n')"
      ],
      "metadata": {
        "id": "0Vrh0J-M6vc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_unigrams = LineSentence(sentence_lemmatized_filepath)"
      ],
      "metadata": {
        "id": "2q7wDjjcgE4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence_unigrams in it.islice(sentences_unigrams, 60, 70):\n",
        "    print(' '.join(sentence_unigrams))\n",
        "    print('')"
      ],
      "metadata": {
        "id": "07D4jzZQgHM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model_filepath = os.path.join(scratch_directory, 'bigram_phrase_model')"
      ],
      "metadata": {
        "id": "8CYl_hdwWPD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "\n",
        "    bigram_phrases = Phrases(sentences_unigrams)\n",
        "    bigram_phrases = Phraser(bigram_phrases)\n",
        "    bigram_phrases.save(bigram_model_filepath)"
      ],
      "metadata": {
        "id": "H0k_JSmfWUdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_phrases = Phraser.load(bigram_model_filepath)"
      ],
      "metadata": {
        "id": "nD_23AzBWgAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_bigrams_filepath = os.path.join(scratch_directory, 'sentence_bigram_phrases_all.txt')"
      ],
      "metadata": {
        "id": "HxMnQIX8XTwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "if execute:\n",
        "    with open(sentences_bigrams_filepath, 'w') as f:\n",
        "        for sentence_unigrams in sentences_unigrams:\n",
        "            sentence_bigrams = ' '.join(bigram_phrases[sentence_unigrams])\n",
        "            f.write(sentence_bigrams + '\\n')"
      ],
      "metadata": {
        "id": "BQBJJsK_XTfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_bigrams = LineSentence(sentences_bigrams_filepath)"
      ],
      "metadata": {
        "id": "tWlC56gIXhb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence_bigrams in it.islice(sentences_bigrams, 60, 70):\n",
        "    print(' '.join(sentence_bigrams))\n",
        "    print('')"
      ],
      "metadata": {
        "id": "OsgvZpPbZanO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_model_filepath = os.path.join(scratch_directory, 'trigram_phrase_model')"
      ],
      "metadata": {
        "id": "DUM4HdVFZhf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "\n",
        "    trigram_phrases = Phrases(sentences_bigrams)\n",
        "    trigram_phrases = Phraser(trigram_phrases)\n",
        "    trigram_phrases.save(trigram_model_filepath)"
      ],
      "metadata": {
        "id": "t4FaYmNjZj_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_phrases = Phraser.load(trigram_model_filepath)"
      ],
      "metadata": {
        "id": "GW-EMxpYZpxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_trigrams_filepath = os.path.join(scratch_directory, 'sentence_trigram_phrases_all.txt')"
      ],
      "metadata": {
        "id": "RTdM18RkZpt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    with open(sentences_trigrams_filepath, 'w') as f:\n",
        "        for sentence_bigrams in sentences_bigrams:\n",
        "            sentence_trigrams = ' '.join(trigram_phrases[sentence_bigrams])\n",
        "            f.write(sentence_trigrams + '\\n')"
      ],
      "metadata": {
        "id": "OEVC1Xt9Z6Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_trigrams = LineSentence(sentences_trigrams_filepath)"
      ],
      "metadata": {
        "id": "LCk86gtvZ58O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence_trigrams in it.islice(sentences_trigrams, 60, 70):\n",
        "    print(' '.join(sentence_trigrams))\n",
        "    print('')"
      ],
      "metadata": {
        "id": "SPDwwRS8aD2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_trigrams_filepath = os.path.join(scratch_directory, 'review_trigrams_all.txt')"
      ],
      "metadata": {
        "id": "SUVhg9pkaDmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    reviews_lemmatized = LineSentence(review_lemmatized_filepath)\n",
        "\n",
        "    with open(review_trigrams_filepath, 'w') as f:\n",
        "        \n",
        "        for review_unigrams in reviews_lemmatized:\n",
        "            review_bigrams = bigram_phrases[review_unigrams]\n",
        "            review_trigrams = trigram_phrases[review_bigrams]\n",
        "\n",
        "            review_trigrams = [\n",
        "                term\n",
        "                for term in review_trigrams\n",
        "                if term not in nlp.Defaults.stop_words\n",
        "                ]\n",
        "\n",
        "            review_trigrams = ' '.join(review_trigrams)\n",
        "            f.write(review_trigrams + '\\n')"
      ],
      "metadata": {
        "id": "M2j6KA-OaHkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_num = 0\n",
        "\n",
        "print('Original:' + '\\n')\n",
        "\n",
        "for review in it.islice(line_review(review_txt_filepath), review_num, review_num+1):\n",
        "    print(review)\n",
        "\n",
        "print('----' + '\\n')\n",
        "print('Transformed:' + '\\n')\n",
        "\n",
        "with open(review_trigrams_filepath) as f:\n",
        "    for review in it.islice(f, review_num, review_num+1):\n",
        "        print(review)"
      ],
      "metadata": {
        "id": "_CPrb4iAaMrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis==2.1.2"
      ],
      "metadata": {
        "id": "62Ix2NRSU9OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary, MmCorpus\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import warnings\n",
        "import pickle"
      ],
      "metadata": {
        "id": "0T5O7zxAU0mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary_filepath = os.path.join(scratch_directory, 'trigram_dict_all.dict')"
      ],
      "metadata": {
        "id": "2BuZEQa3VK03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    reviews_trigrams = LineSentence(review_trigrams_filepath)\n",
        "    dictionary_trigrams = Dictionary(reviews_trigrams)\n",
        "    dictionary_trigrams.filter_extremes(no_below=20, no_above=0.4)\n",
        "    dictionary_trigrams.compactify()\n",
        "    dictionary_trigrams.save(dictionary_filepath)  "
      ],
      "metadata": {
        "id": "UH1_3yKsVNiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary_trigrams = Dictionary.load(dictionary_filepath)"
      ],
      "metadata": {
        "id": "JCue4KeEWITd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpus_filepath = os.path.join(scratch_directory, 'bow_trigrams_corpus_all.mm')"
      ],
      "metadata": {
        "id": "mAWzpJIvWN18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bow_generator(filepath):\n",
        "   \n",
        "    for review in LineSentence(filepath):\n",
        "        yield dictionary_trigrams.doc2bow(review)"
      ],
      "metadata": {
        "id": "xYOWcYmXWQHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    MmCorpus.serialize(bow_corpus_filepath, bow_generator(review_trigrams_filepath))"
      ],
      "metadata": {
        "id": "MpCBPAxkWVWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_bow_corpus = MmCorpus(bow_corpus_filepath)"
      ],
      "metadata": {
        "id": "xa0hDGqVXViJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model_filepath = os.path.join(scratch_directory, 'lda_model_all')"
      ],
      "metadata": {
        "id": "0_8toQmDXXqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "execute = False\n",
        "\n",
        "if execute:\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')\n",
        "        lda = LdaMulticore(trigram_bow_corpus, num_topics=50, id2word=dictionary_trigrams, workers=7)\n",
        "    \n",
        "    lda.save(lda_model_filepath)"
      ],
      "metadata": {
        "id": "-1s645atXbGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LdaMulticore.load(lda_model_filepath)"
      ],
      "metadata": {
        "id": "r8eprIBlXnWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_topic(topic_number, topn=25):\n",
        "    print(f'{\"term\":20} {\"frequency\"}' + '\\n')\n",
        "\n",
        "    for term, frequency in lda.show_topic(topic_number, topn=topn):\n",
        "        print(f'{term:20} {round(frequency, 3):.3f}')"
      ],
      "metadata": {
        "id": "LEKeFThHXp37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explore_topic(topic_number=20, topn=5)"
      ],
      "metadata": {
        "id": "FCrDQf-vXto7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_names = {\n",
        "    0: 'place1',\n",
        "    1: 'sauce',\n",
        "    2: 'place2',\n",
        "    3: 'time',\n",
        "    4: 'service',\n",
        "    5: 'seafood1',\n",
        "    6: 'reservation',\n",
        "    7: 'taste',\n",
        "    8: 'donut',\n",
        "    9: 'vietnam',\n",
        "    10: 'orders1',\n",
        "    11: 'time',\n",
        "    12: 'salad',\n",
        "    13: 'order2', #\n",
        "    14: 'burgers & fries',\n",
        "    15: 'mexican',\n",
        "    16: 'order3',\n",
        "    17: 'seafood2',\n",
        "    18: 'staff',\n",
        "    19: 'atmosphere',\n",
        "    20: 'chip',\n",
        "    21: 'bar vibe', #\n",
        "    22: 'meal experience', #\n",
        "    23: 'slow service',\n",
        "    24: 'brunch',\n",
        "    25: 'portion sizes',\n",
        "    26: 'beer, wings, sports',\n",
        "    27: 'breakfast',\n",
        "    28: 'miscellaneous',\n",
        "    29: 'non-English',\n",
        "    30: 'deli',\n",
        "    31: 'barbecue',\n",
        "    32: 'local business',\n",
        "    33: 'miscellaneous',\n",
        "    34: 'hole-in-the-wall',\n",
        "    35: 'asian',\n",
        "    36: 'specials',\n",
        "    37: 'coffeeshop',\n",
        "    38: 'prices',\n",
        "    39: 'flavor & texture',\n",
        "    40: 'noodles',\n",
        "    41: 'canadian',\n",
        "    42: 'highly recommended',\n",
        "    43: 'sushi',\n",
        "    44: 'ordering',\n",
        "    45: 'mediterranean',\n",
        "    46: 'decent value',\n",
        "    47: 'cleanliness',\n",
        "    48: 'lobster',\n",
        "    49: 'seafood'\n",
        "    }"
      ],
      "metadata": {
        "id": "ta98ld8_LyD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_names_filepath = os.path.join(scratch_directory, 'topic_names.pkl')\n",
        "\n",
        "with open(topic_names_filepath, 'wb') as f:\n",
        "    pickle.dump(topic_names, f)"
      ],
      "metadata": {
        "id": "AUCZ3pt8OCzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LDAvis_data_filepath = os.path.join(scratch_directory, 'ldavis_prepared')"
      ],
      "metadata": {
        "id": "Xzz717bXOGRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Por hacer**\n",
        "\n",
        "* Añadir comentarios\n",
        "* Incrustaciones de palabra con Word2vec\n",
        "* Visualizaciones\n",
        "* Álgebra de palabras"
      ],
      "metadata": {
        "id": "pq0aqfH1YqAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referencias**\n",
        "\n",
        "* https://spacy.io/\n",
        "* https://radimrehurek.com/gensim/\n",
        "* https://github.com/pwharrison/modern-nlp-in-python-2019/blob/master/notebooks/Modern_NLP_in_Python.ipynb"
      ],
      "metadata": {
        "id": "QClEkJKWaKpp"
      }
    }
  ]
}