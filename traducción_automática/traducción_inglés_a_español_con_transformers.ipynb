{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoBRdgz/inteligencia_artificial/blob/main/traducci%C3%B3n_autom%C3%A1tica/traducci%C3%B3n_ingl%C3%A9s_a_espa%C3%B1ol_con_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_2xw8hdLFK1"
      },
      "source": [
        "# Traducción Automática de Inglés a Español\n",
        "\n",
        "**Objetivo:**\n",
        "\n",
        "Entrenar un Transformador *seq2seq* (secuencia a secuencia) para resolver una tarea de traducción automática de inglés a español.\n",
        "\n",
        "Este problema es conocido por sus siglas en inglés como NMT (*Neural Machine Translation*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2GNvC9bLFK_"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "En este *notebook* se diseña un modelo de tipo Transformador de secuencia a secuencia utilizando la API funcional de Tensorflow, que será entrenado en una tarea de traducción automática de inglés a español.\n",
        "\n",
        "Se implementarán los siguientes puntos:\n",
        "\n",
        "- Preparar los datos para entrenar un modelo *seq2seq* donde la secuencia de entrada es una oración en inglés y la secuencia de salida será la oración traducida a español\n",
        "- Vectorizar el texto usando la clase `TextVectorization` de Keras\n",
        "- Implementar una clase `PositionalEmbedding`\n",
        "- Implementar una clase `TransformerEncoder`\n",
        "- Implementar una clase `TransformerDecoder`\n",
        "- Entrenar el modelo para resolver una tarea de traducción automática\n",
        "- Utilizar el modelo para generar traducciones a oraciones que no fueron utilizadas durante el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Características de Hardware\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2KuEEw_XNWQ",
        "outputId": "11c14c31-3519-4502-aa64-cfa0d7c9870b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan  9 03:37:07 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0    31W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA7WpDtgLFLB"
      },
      "source": [
        "## Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zXmG5NuNLFLC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import string\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRLSm6dHLFLE"
      },
      "source": [
        "## Conjunto de datos\n",
        "\n",
        "En el siguiente enlace se pueden encontrar diversos conjuntos de datos de oraciones en inglés traducidas a varios idiomas: https://www.manythings.org/anki/.\n",
        "\n",
        "En este caso se utilizará el conjunto de datos correspondiente a inglés - español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tCvTawS4LFLE"
      },
      "outputs": [],
      "source": [
        "text_file = tf.keras.utils.get_file(fname=\"spa-eng.zip\",\n",
        "                                    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "                                    extract=True)\n",
        "\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW1efeD_LFLF"
      },
      "source": [
        "## Preprocesamiento de datos\n",
        "\n",
        "Cada línea del conjunto de datos contiene una oración en inglés y su correspondiente oración traducida a español.\n",
        "La oración en inglés es la secuencia de origen y la oración en español es la secuencia de destino.\n",
        "Anteponemos el *token* `\"[start]\"` y agregamos el token `\"[end]\"` a la oración en español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rzPrCXHvLFLF"
      },
      "outputs": [],
      "source": [
        "# Abrir el archivo de texto especificado\n",
        "with open(text_file) as f:\n",
        "    # Leer las líneas del archivo y dividirlas en una lista\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "# Inicializar una lista para almacenar los pares de texto\n",
        "text_pairs = []\n",
        "\n",
        "# Para cada línea en el archivo\n",
        "for line in lines:\n",
        "    # Dividir en los idiomas inglés y español\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    # Agregar marcadores de inicio y fin al texto en español\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    # Agregar los pares de texto a la lista\n",
        "    text_pairs.append((eng, spa))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUqX5N3jLFLG"
      },
      "source": [
        "Así es como se ven los pares de oraciones (inglés, español) generados por la celda previa para 5 registros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9j2HpqzSLFLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118f7e8b-1773-4de2-8140-484dc051048c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I will be in high school next April.', '[start] Estaré en secundaria el próximo abril. [end]') \n",
            "\n",
            "('Tom has bad table manners.', '[start] Tom tiene malos modales en la mesa. [end]') \n",
            "\n",
            "('How is everything?', '[start] ¿Cómo va todo? [end]') \n",
            "\n",
            "('I was at home almost all day yesterday.', '[start] Ayer estuve casi todo el día en casa. [end]') \n",
            "\n",
            "(\"That was the tiniest cockroach I've ever seen in my life.\", '[start] Aquella fue la cucaracha más pequeña que he visto en mi vida. [end]') \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2oBKmWtLFLH"
      },
      "source": [
        "Ahora, se particionan los pares de oraciones en los conjuntos de entrenamiento, validación, y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "udGlLRQuLFLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d3ecf4-ad77-432d-cbdb-cd56b3f9ee2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 118964 Pares totales (inglés, español)\n",
            "[INFO] 83276 Pares en el conjunto de entrenamiento\n",
            "[INFO] 17844 Pares en el conjunto de validación\n",
            "[INFO] 17844 Pares en el conjunto de prueba\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"[INFO] {len(text_pairs)} Pares totales (inglés, español)\")\n",
        "print(f\"[INFO] {len(train_pairs)} Pares en el conjunto de entrenamiento\")\n",
        "print(f\"[INFO] {len(val_pairs)} Pares en el conjunto de validación\")\n",
        "print(f\"[INFO] {len(test_pairs)} Pares en el conjunto de prueba\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-DxasW0LFLI"
      },
      "source": [
        "## Vectorización de texto\n",
        "\n",
        "Se utilizan dos instancias de la clase `TextVectorization` para vectorizar los datos de texto (una para inglés y otra para español), es decir, se convierten las cadenas originales en texto a secuencias de números enteros donde cada entero representa el índice de una palabra en un vocabulario.\n",
        "\n",
        "La clase en inglés utilizará la estandarización de cadenas predeterminada (eliminar los caracteres de puntuación) y el enfoque de separación (separar por espacios en blanco), mientras que en la clase en español se utilizará una estandarización personalizada, donde se agregará el caracter `\"¿\"` al conjunto de caracteres de puntuación **a eliminar**.\n",
        "\n",
        "Importante: La limpieza de datos propuesta es sólo con fines didácticos, pues en caso de tener como objetivo el despliegue a producción de un modelo de traducción automática, no se recomendaría eliminar los caracteres de puntuación en independencia del idioma que se esté modelando. En su lugar, convertir cada carácter de puntuación a su propio *token*."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    \"\"\"Realiza la estandarización de una cadena de entrada.\n",
        "    Primero, convierte la cadena a minúsculas. Luego, elimina todos los caracteres especificados en la variable\n",
        "    global `strip_chars` utilizando expresiones regulares.\n",
        "\n",
        "    Argumentos:\n",
        "        input_string: Una cadena de tensores, la cadena de entrada.\n",
        "\n",
        "    Devuelve:\n",
        "        Una cadena de tensores, la cadena estandarizada.\n",
        "    \"\"\"\n",
        "    strip_chars = string.punctuation + \"¿\"\n",
        "    strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "    strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "    # Convertir la cadena a minúsculas\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    # Eliminar los caracteres especificados en `strip_chars` utilizando expresiones regulares\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")"
      ],
      "metadata": {
        "id": "k3d-aSn2nMEC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cfNvmnyWLFLJ"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "eng_vectorization = TextVectorization(max_tokens=vocab_size,\n",
        "                                      output_mode=\"int\",\n",
        "                                      output_sequence_length=sequence_length)\n",
        "\n",
        "spa_vectorization = TextVectorization(max_tokens=vocab_size,\n",
        "                                      output_mode=\"int\",\n",
        "                                      output_sequence_length=sequence_length + 1,\n",
        "                                      standardize=custom_standardization)\n",
        "\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ14pkFkLFLK"
      },
      "source": [
        "Ahora, se formatearán las particiones de datos.\n",
        "\n",
        "En cada paso de entrenamiento, el modelo buscará predecir las palabras objetivo (*targets*) N+1 y consecuentes usando la oración de origen (*inputs*) y las palabras objetivo 0 a N.\n",
        "\n",
        "El conjunto de datos de entrenamiento generará una tupla `(inputs, targets)`, donde:\n",
        "\n",
        "- `inputs` es un diccionario con las llaves `encoder_inputs` y `decoder_inputs`. `encoder_inputs` es la oración de origen vectorizada y `decoder_inputs` es la oración de destino \"hasta ahora\", es decir, las palabras 0 a N utilizadas para predecir la palabra N+1 (y consecuentes) en la oración de destino.\n",
        "- `target` es la oración de destino desplazada por un paso: proporciona las siguientes palabras en la oración de destino, o dicho de otra forma, lo que el modelo intentará predecir."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, spa):\n",
        "    \"\"\"Formatear el conjunto de datos para entrenamiento.\n",
        "    Argumentos:\n",
        "    eng: Un tensor, el tensor de entrada del encoder.\n",
        "    spa: Un tensor, el tensor de entrada del decoder.\n",
        "\n",
        "    Devuelve:\n",
        "        Un par de tensores, el tensor de entrada para el modelo y el tensor de salida para el modelo.\n",
        "    \"\"\"\n",
        "    # Vectorizar la entrada del encoder\n",
        "    eng = eng_vectorization(eng)\n",
        "    # Vectorizar la entrada del decoder\n",
        "    spa = spa_vectorization(spa)\n",
        "    # Devolver el par de tensores formateados\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])"
      ],
      "metadata": {
        "id": "3CVh-L0qrPcs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6SQN-WDWLFLL"
      },
      "outputs": [],
      "source": [
        "def make_dataset(pairs):\n",
        "    \"\"\"Crear un dataset a partir de pares de texto en inglés y español.\n",
        "    Argumentos:\n",
        "    pairs: Una lista de tuplas, cada tupla contiene un texto en inglés y otro en español.\n",
        "\n",
        "    Devuelve:\n",
        "        Un dataset, el dataset creado.\n",
        "    \"\"\"\n",
        "    # Desempaquetar la lista de pares en dos listas, una con textos en inglés y otra con textos en español\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    # Convertir las listas a listas de Python\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    # Crear un dataset a partir de las listas de textos\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    # Agrupar los elementos del dataset en batches\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # Aplicar la función de formateo al dataset\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    # Mezclar los elementos del dataset, prefetchear 16 elementos y cachear los resultados\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "3dleuFqqrTzo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYJ_vLRrLFLL"
      },
      "source": [
        "Validación de dimensiones de la secuencia. (Se toman lotes de 64 pares y todas las secuencias tienen 20 pasos):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o9h7EDnULFLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b75db92-7b35-4be7-8d9f-97846efe672a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] inputs['encoder_inputs'].shape: (64, 20)\n",
            "[INFO] inputs['decoder_inputs'].shape: (64, 20)\n",
            "[INFO] targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"[INFO] inputs['encoder_inputs'].shape: {inputs['encoder_inputs'].shape}\")\n",
        "    print(f\"[INFO] inputs['decoder_inputs'].shape: {inputs['decoder_inputs'].shape}\")\n",
        "    print(f\"[INFO] targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtcBVOkwLFLM"
      },
      "source": [
        "## Construcción del modelo\n",
        "\n",
        "Nuestro Transformador de secuencia a secuencia consiste en un `TransformerEncoder` y un `TransformerDecoder` encadenados juntos. Para que el modelo sea consciente del orden de las palabras, también usamos una capa `PositionalEmbedding`.\n",
        "\n",
        "La secuencia fuente se pasará al `TransformerEncoder`, que producirá una nueva representación de la misma. Esta nueva representación se pasará al `TransformerDecoder`, junto con la secuencia de destino hasta el momento (palabras de destino 0 a N). El 'TransformerDecoder' intentará predecir las siguientes palabras en la secuencia de destino (N+1 y más allá).\n",
        "\n",
        "Un detalle clave que hace esto posible es el enmascaramiento causal (consulte el método `get_causal_attention_mask()` en `TransformerDecoder`). El `TransformerDecoder` ve las secuencias completas a la vez y, por lo tanto, debemos asegurarnos de que solo use información de los tokens de destino 0 a N al predecir el token N+1 (de lo contrario, podría usar información del futuro, lo que daría como resultado un modelo que no se puede utilizar en el momento de la inferencia)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int, **kwargs):\n",
        "        \"\"\"Inicializar la clase PositionalEmbedding.\n",
        "        Argumentos:\n",
        "            sequence_length: Entero, la longitud de la secuencia.\n",
        "            vocab_size: Entero, el tamaño del vocabulario.\n",
        "            embed_dim: Entero, el tamaño de las incrustaciones.\n",
        "            **kwargs: Otros argumentos clave.\n",
        "        \"\"\"\n",
        "        # Inicializar la clase\n",
        "        super().__init__(**kwargs)\n",
        "        # Inicializar las incrustaciones para los tokens y las posiciones\n",
        "        self.token_embeddings = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
        "        # Guardar los atributos de la clase\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Realizar el pase hacia adelante de la capa PositionalEmbedding.\n",
        "\n",
        "        Argumentos:\n",
        "            inputs: Un tensor, el tensor de entrada de la capa.\n",
        "\n",
        "        Devuelve:\n",
        "            Un tensor, el tensor de salida de la capa.\n",
        "        \"\"\"\n",
        "        # Obtener la longitud de los inputs\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        # Generar un tensor con las posiciones\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        # Incorporar incrustaciones a los tokens\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        # Incorporar incrustaciones a las posiciones\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        # Sumar las incrustaciones de los tokens y posiciones\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
        "        \"\"\"Calcular la máscara para la capa PositionalEmbedding.\n",
        "\n",
        "        Argumentos:\n",
        "            inputs: Un tensor, el tensor de entrada de la capa.\n",
        "            mask: Un tensor, la máscara a aplicar a la capa.\n",
        "\n",
        "        Devuelve:\n",
        "            Un tensor, la máscara para la capa.\n",
        "        \"\"\"\n",
        "        # Devolver una máscara que indique dónde hay tokens presentes\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Devuelve la configuración de la capa PositionalEmbedding.\n",
        "\n",
        "        Devuelve:\n",
        "            Un diccionario, la configuración de la capa.\n",
        "        \"\"\"\n",
        "        # Obtener la configuración de la clase padre\n",
        "        config = super().get_config()\n",
        "        # Actualizar la configuración con los atributos de esta clase\n",
        "        config.update({\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"vocab_size\": self.vocab_size,\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        # Devolver la configuración actualizada\n",
        "        return config"
      ],
      "metadata": {
        "id": "UDfztXVRyq6g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim: int, dense_dim: int, num_heads: int, **kwargs):\n",
        "        \"\"\"Inicializar la capa TransformerEncoder.\n",
        "\n",
        "        Argumentos:\n",
        "            embed_dim: Entero, el tamaño de las incrustaciones.\n",
        "            dense_dim: Entero, el tamaño de la capa densa.\n",
        "            num_heads: Entero, el número de cabezas de atención.\n",
        "        \"\"\"\n",
        "        # Inicializar la clase\n",
        "        super().__init__(**kwargs)\n",
        "        # Asignar el tamaño de las incrustaciones a una variable de instancia\n",
        "        self.embed_dim = embed_dim\n",
        "        # Asignar el tamaño de la capa densa a una variable de instancia\n",
        "        self.dense_dim = dense_dim\n",
        "        # Asignar el número de cabezas de atención a una variable de instancia\n",
        "        self.num_heads = num_heads\n",
        "        # Crear una capa de atención multi-cabeza\n",
        "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        # Crear una secuencia de capas densas\n",
        "        self.dense_proj = tf.keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                                               layers.Dense(embed_dim),])\n",
        "        # Crear una capa de normalización de capa\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        # Crear otra capa de normalización de capa\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        # Establecer la propiedad de soporte de máscara en verdadero\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
        "        \"\"\"Realizar el pase hacia adelante de la capa TransformerEncoder.\n",
        "\n",
        "        Argumentos:\n",
        "            inputs: Un tensor, el tensor de entrada de la capa.\n",
        "            mask: Un tensor, la máscara a aplicar a la atención.\n",
        "\n",
        "        Devuelve:\n",
        "            Un tensor, el tensor de salida de la capa.\n",
        "        \"\"\"\n",
        "        # Calcular la máscara de atención a partir de la máscara dada\n",
        "        attention_mask = self.attention.compute_mask(mask)\n",
        "        # Calcular la salida de atención a partir de las entradas y la máscara\n",
        "        attention_output = self.attention(query=inputs,\n",
        "                                          value=inputs,\n",
        "                                          key=inputs,\n",
        "                                          attention_mask=attention_mask)\n",
        "        # Aplicar la primera capa de normalización de capa y sumarla a la salida de atención\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        # Calcular la salida de la secuencia de capas densas a partir de la entrada proyectada\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        # Aplicar la segunda capa de normalización de capa y sumarla a la salida proyectada\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Devuelve la configuración de la capa TransformerEncoder.\n",
        "\n",
        "        Devuelve:\n",
        "            Un diccionario, la configuración de la capa.\n",
        "        \"\"\"\n",
        "        # Obtener la configuración de la clase base\n",
        "        config = super().get_config()\n",
        "        # Actualizar la configuración con las variables de instancia de la capa TransformerEncoder\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        # Devolver la configuración actualizada\n",
        "        return config"
      ],
      "metadata": {
        "id": "uKNJ7v0pkfFA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim: int, latent_dim: int, num_heads: int, **kwargs):\n",
        "        \"\"\"Inicializar la capa TransformerDecoder.\n",
        "\n",
        "        Argumentos:\n",
        "            embed_dim: Entero, el tamaño de las incrustaciones.\n",
        "            latent_dim: Entero, el tamaño del espacio latente.\n",
        "            num_heads: Entero, el número de cabezas de atención.\n",
        "        \"\"\"\n",
        "        # Inicializar la clase\n",
        "        super().__init__(**kwargs)\n",
        "        # Guardar la dimensión de las incrustaciones, la dimensión latente y el número de cabezas de atención\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Crear dos capas de atención multi-cabeza\n",
        "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        # Crear una secuencia de capas densas que proyectará el tensor de salida\n",
        "        self.dense_proj = tf.keras.Sequential([layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                                               layers.Dense(embed_dim)])\n",
        "        # Crear tres capas de normalización\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        # Indicar que esta capa soporta el uso de máscaras\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, encoder_outputs: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
        "        \"\"\"Realiza el pase hacia adelante del decodificador Transformer.\n",
        "\n",
        "        Argumentos:\n",
        "            inputs: Un tensor, el tensor de entrada del decodificador.\n",
        "            encoder_outputs: Un tensor, el tensor de salida del codificador.\n",
        "            mask: Un tensor, la máscara a aplicar a la capa.\n",
        "\n",
        "        Devuelve:\n",
        "            Un tensor, el tensor de salida del decodificador.\n",
        "        \"\"\"\n",
        "        # Obtener la máscara de atención causal\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        # Aplicar la máscara de padding si se proporciona\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        # Realizar el pase hacia adelante de la primera atención múltiple\n",
        "        attention_output_1 = self.attention_1(query=inputs,\n",
        "                                              value=inputs,\n",
        "                                              key=inputs,\n",
        "                                              attention_mask=causal_mask)\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        # Realizar el pase hacia adelante de la segunda atención múltiple\n",
        "        attention_output_2 = self.attention_2(query=out_1,\n",
        "                                              value=encoder_outputs,\n",
        "                                              key=encoder_outputs,\n",
        "                                              attention_mask=padding_mask)\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        # Realizar el pase hacia adelante de la capa densa\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "\n",
        "        # Devolver el pase hacia adelante final\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Genera una máscara causal para la atención en el decodificador del Transformer.\n",
        "           La máscara se utilizará para restringir la atención del decodificador a solo\n",
        "           los tokens anteriores al token actual durante el proceso de pase hacia adelante.\n",
        "\n",
        "        Argumentos:\n",
        "            inputs: Un tensor, el tensor de entrada del decodificador.\n",
        "\n",
        "        Devuelve:\n",
        "            Un tensor, la máscara causal para la atención en el decodificador.\n",
        "        \"\"\"\n",
        "        # Obtener las dimensiones del tensor de entrada\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        # Generar una máscara con una diagonal de 1's y 0's\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        # Duplicar la máscara para cada elemento del batch\n",
        "        mult = tf.concat([tf.expand_dims(batch_size, -1),\n",
        "                          tf.constant([1, 1], dtype=tf.int32)],\n",
        "                        axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def get_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Devuelve la configuración de la capa TransformerDecoder.\n",
        "\n",
        "        Devuelve:\n",
        "            Un diccionario, la configuración de la capa.\n",
        "        \"\"\"\n",
        "        # Obtener la configuración de la clase base\n",
        "        config = super().get_config()\n",
        "        # Actualizar la configuración con las variables de instancia de la capa TransformerDecoder\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"latent_dim\": self.latent_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        # Devolver la configuración actualizada\n",
        "        return config"
      ],
      "metadata": {
        "id": "59PPpzc7UDQh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZljumWTLFLN"
      },
      "source": [
        "## Ensamblaje del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EZxFjojaLFLN"
      },
      "outputs": [],
      "source": [
        "num_heads = 8\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "\n",
        "# Crear un tensor de entrada para el encoder con una forma dinámica (None) y tipo de datos \"int64\"\n",
        "encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "\n",
        "# Aplicar la capa PositionalEmbedding al tensor de entrada del encoder\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "\n",
        "# Aplicar la capa TransformerEncoder al tensor resultante\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "\n",
        "# Crear un modelo para el encoder utilizando el tensor de entrada y el tensor de salida\n",
        "encoder = tf.keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "# Crear un tensor de entrada para el decoder con una forma dinámica (None) y tipo de datos \"int64\"\n",
        "decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "\n",
        "# Crear un tensor de entrada para el estado del decoder con una forma dinámica (None) y tamaño de incrustación\n",
        "encoded_seq_inputs = tf.keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "\n",
        "# Aplicar la capa PositionalEmbedding al tensor de entrada del decoder\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "\n",
        "# Aplicar la capa TransformerDecoder al tensor resultante y al tensor de estado del decoder\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "\n",
        "# Aplicar una capa de Dropout al tensor resultante\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Aplicar una capa Dense con función de activación \"softmax\" al tensor resultante\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "# Crear un modelo para el decoder utilizando los tensores de entrada y el tensor de salida\n",
        "decoder = tf.keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "# Obtener el tensor de salida del decoder utilizando los tensores de entrada del decoder y el encoder\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "# Crear un modelo con las entradas del codificador y del decodificador, y las salidas del decodificador\n",
        "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg0iBEjCLFLO"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Se utiliza el *accuracy* como una forma rápida de monitorear el progreso del entrenamiento en los datos de validación, sin embargo, se debe tener en cuenta que para problemas de traducción automática generalmente se utiliza el puntaje BLEU, así como otras métricas en lugar del *accuracy*.\n",
        "\n",
        "Se recomienda dejar el modelo entrenando durante al menos 30 épocas."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "\n",
        "ckpt = ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "                       monitor='val_loss',\n",
        "                       verbose=1,\n",
        "                       save_best_only=True)\n",
        "transformer.compile(\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "transformer.summary()"
      ],
      "metadata": {
        "id": "cfIU5XRAsKu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c66c0c2-bf99-4f2f-9584-88dfd49504bf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n",
            " alEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
            " erEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19,960,216\n",
            "Trainable params: 19,960,216\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bJPPanK_LFLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c83a368-8bce-4deb-bd09-d12c29554b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.7345 - accuracy: 0.3955\n",
            "Epoch 1: val_loss improved from inf to 1.36802, saving model to weights.01-1.37.h5\n",
            "1302/1302 [==============================] - 96s 70ms/step - loss: 1.7345 - accuracy: 0.3955 - val_loss: 1.3680 - val_accuracy: 0.4906\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.3684 - accuracy: 0.5217\n",
            "Epoch 2: val_loss improved from 1.36802 to 1.18675, saving model to weights.02-1.19.h5\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.3684 - accuracy: 0.5217 - val_loss: 1.1868 - val_accuracy: 0.5584\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.2022 - accuracy: 0.5766\n",
            "Epoch 3: val_loss improved from 1.18675 to 1.10110, saving model to weights.03-1.10.h5\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.2022 - accuracy: 0.5766 - val_loss: 1.1011 - val_accuracy: 0.5944\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.1061 - accuracy: 0.6111\n",
            "Epoch 4: val_loss improved from 1.10110 to 1.05103, saving model to weights.04-1.05.h5\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.1061 - accuracy: 0.6111 - val_loss: 1.0510 - val_accuracy: 0.6185\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.0545 - accuracy: 0.6349\n",
            "Epoch 5: val_loss improved from 1.05103 to 1.03333, saving model to weights.05-1.03.h5\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.0545 - accuracy: 0.6349 - val_loss: 1.0333 - val_accuracy: 0.6243\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.0228 - accuracy: 0.6508\n",
            "Epoch 6: val_loss improved from 1.03333 to 1.01517, saving model to weights.06-1.02.h5\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.0228 - accuracy: 0.6508 - val_loss: 1.0152 - val_accuracy: 0.6376\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.0003 - accuracy: 0.6636\n",
            "Epoch 7: val_loss did not improve from 1.01517\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.0003 - accuracy: 0.6636 - val_loss: 1.0191 - val_accuracy: 0.6388\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.9820 - accuracy: 0.6742\n",
            "Epoch 8: val_loss improved from 1.01517 to 1.00655, saving model to weights.08-1.01.h5\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.9820 - accuracy: 0.6742 - val_loss: 1.0066 - val_accuracy: 0.6440\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.9655 - accuracy: 0.6828\n",
            "Epoch 9: val_loss improved from 1.00655 to 1.00387, saving model to weights.09-1.00.h5\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 0.9655 - accuracy: 0.6828 - val_loss: 1.0039 - val_accuracy: 0.6467\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.9504 - accuracy: 0.6907\n",
            "Epoch 10: val_loss did not improve from 1.00387\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.9504 - accuracy: 0.6907 - val_loss: 1.0091 - val_accuracy: 0.6471\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.9364 - accuracy: 0.6974\n",
            "Epoch 11: val_loss improved from 1.00387 to 1.00367, saving model to weights.11-1.00.h5\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 0.9364 - accuracy: 0.6974 - val_loss: 1.0037 - val_accuracy: 0.6510\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.9237 - accuracy: 0.7040\n",
            "Epoch 12: val_loss improved from 1.00367 to 1.00287, saving model to weights.12-1.00.h5\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 0.9237 - accuracy: 0.7040 - val_loss: 1.0029 - val_accuracy: 0.6523\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.9112 - accuracy: 0.7093\n",
            "Epoch 13: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.9112 - accuracy: 0.7093 - val_loss: 1.0075 - val_accuracy: 0.6531\n",
            "Epoch 14/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8998 - accuracy: 0.7144\n",
            "Epoch 14: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 0.8998 - accuracy: 0.7144 - val_loss: 1.0130 - val_accuracy: 0.6527\n",
            "Epoch 15/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8878 - accuracy: 0.7195\n",
            "Epoch 15: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 0.8878 - accuracy: 0.7195 - val_loss: 1.0121 - val_accuracy: 0.6563\n",
            "Epoch 16/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8766 - accuracy: 0.7242\n",
            "Epoch 16: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.8766 - accuracy: 0.7242 - val_loss: 1.0178 - val_accuracy: 0.6573\n",
            "Epoch 17/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8658 - accuracy: 0.7280\n",
            "Epoch 17: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.8658 - accuracy: 0.7280 - val_loss: 1.0162 - val_accuracy: 0.6578\n",
            "Epoch 18/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8553 - accuracy: 0.7320\n",
            "Epoch 18: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 0.8553 - accuracy: 0.7320 - val_loss: 1.0175 - val_accuracy: 0.6566\n",
            "Epoch 19/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8468 - accuracy: 0.7359\n",
            "Epoch 19: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.8468 - accuracy: 0.7359 - val_loss: 1.0216 - val_accuracy: 0.6516\n",
            "Epoch 20/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8370 - accuracy: 0.7391\n",
            "Epoch 20: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.8370 - accuracy: 0.7391 - val_loss: 1.0336 - val_accuracy: 0.6560\n",
            "Epoch 21/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8284 - accuracy: 0.7427\n",
            "Epoch 21: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 0.8284 - accuracy: 0.7427 - val_loss: 1.0348 - val_accuracy: 0.6567\n",
            "Epoch 22/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8193 - accuracy: 0.7458\n",
            "Epoch 22: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 0.8193 - accuracy: 0.7458 - val_loss: 1.0406 - val_accuracy: 0.6551\n",
            "Epoch 23/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8108 - accuracy: 0.7490\n",
            "Epoch 23: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 0.8108 - accuracy: 0.7490 - val_loss: 1.0354 - val_accuracy: 0.6576\n",
            "Epoch 24/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8023 - accuracy: 0.7526\n",
            "Epoch 24: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 0.8023 - accuracy: 0.7526 - val_loss: 1.0533 - val_accuracy: 0.6565\n",
            "Epoch 25/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.7948 - accuracy: 0.7551\n",
            "Epoch 25: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 0.7948 - accuracy: 0.7551 - val_loss: 1.0552 - val_accuracy: 0.6555\n",
            "Epoch 26/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.7872 - accuracy: 0.7579\n",
            "Epoch 26: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 0.7872 - accuracy: 0.7579 - val_loss: 1.0523 - val_accuracy: 0.6576\n",
            "Epoch 27/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.7801 - accuracy: 0.7603\n",
            "Epoch 27: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.7801 - accuracy: 0.7603 - val_loss: 1.0601 - val_accuracy: 0.6561\n",
            "Epoch 28/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.7730 - accuracy: 0.7626\n",
            "Epoch 28: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.7730 - accuracy: 0.7626 - val_loss: 1.0553 - val_accuracy: 0.6600\n",
            "Epoch 29/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.7669 - accuracy: 0.7653\n",
            "Epoch 29: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.7669 - accuracy: 0.7653 - val_loss: 1.0650 - val_accuracy: 0.6604\n",
            "Epoch 30/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.7607 - accuracy: 0.7679\n",
            "Epoch 30: val_loss did not improve from 1.00287\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 0.7607 - accuracy: 0.7679 - val_loss: 1.0659 - val_accuracy: 0.6582\n",
            "CPU times: user 27min 20s, sys: 1min 56s, total: 29min 16s\n",
            "Wall time: 57min\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f508d124df0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "%%time\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[ckpt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy6ZRlc5LFLO"
      },
      "source": [
        "## Decodificación de oraciones de prueba\n",
        "\n",
        "Finalmente, para traducir oraciones nuevas, simplemente se ingresa al modelo la oración en inglés vectorizada, así como el token de destino `\"[start]\"`, luego se genera repetidamente el siguiente token hasta llegar al token `\"[end]\"`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_sentence):\n",
        "    \"\"\"Decodifica una secuencia de entrada en una frase en español.\n",
        "\n",
        "    Argumentos:\n",
        "        input_sentence: Una cadena, la frase en inglés a ser decodificada.\n",
        "\n",
        "    Devuelve:\n",
        "        Una cadena, la frase en español decodificada.\n",
        "    \"\"\"\n",
        "    # Tokenizar la frase de entrada\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    # Inicializar la frase decodificada con el marcador de inicio\n",
        "    decoded_sentence = \"[start]\"\n",
        "    # Iterar hasta la longitud máxima de la frase decodificada\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        # Tokenizar la frase decodificada hasta el momento\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        # Obtener las predicciones del modelo para la siguiente palabra\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        # Tomar la palabra con mayor probabilidad\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        # Convertir el índice de la palabra en la palabra en sí\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        # Añadir la palabra a la frase decodificada\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        # Si se encontró el marcador de fin, terminar la decodificación\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    # Devuelve la oración decodificada\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "1lyJqgCg5joc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer el tamaño máximo de la frase decodificada\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "# Crear un diccionario que permita buscar el índice del vocabulario en español a partir de un token\n",
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))"
      ],
      "metadata": {
        "id": "NHjI0vUV5on5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de 30 épocas se obtienen los siguientes resultados:"
      ],
      "metadata": {
        "id": "TQYRHiTOYUZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionamos algunos textos de prueba de inglés\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "# Hacemos 10 iteraciones para elegir una oración de entrada al azar y traducirla\n",
        "for _ in range(10):\n",
        "    # Elegimos una oración de entrada al azar\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    # Traducimos la oración de entrada\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    # Imprimimos la oración de entrada y la traducción\n",
        "    print(input_sentence, translated, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQJYNLF7X_tk",
        "outputId": "c50f9523-25e3-410f-8c0f-543c2fe40427"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The girl has a scarf around her neck. [start] la niña ha roto la [UNK] [end] \n",
            "\n",
            "There's no need to hurry. We have plenty of time. [start] no tienes suerte de casarse tenemos mucho tiempo [end] \n",
            "\n",
            "I'm eating now. [start] estoy comiendo ahora [end] \n",
            "\n",
            "I do not allow sleeping in class. [start] no [UNK] la del trabajo [end] \n",
            "\n",
            "I just want to be a normal person. [start] solo quiero ser una normal [end] \n",
            "\n",
            "Is this Tom's? [start] esto es de tom [end] \n",
            "\n",
            "You're a weird kid. [start] eres un niño extraño [end] \n",
            "\n",
            "You can't stay in here all day. [start] no puedes quedar aquí todo el día [end] \n",
            "\n",
            "Please remember that. [start] por favor [UNK] eso [end] \n",
            "\n",
            "I have no idea what Tom's problem is. [start] no tengo idea de cuál es el problema de tom [end] \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}