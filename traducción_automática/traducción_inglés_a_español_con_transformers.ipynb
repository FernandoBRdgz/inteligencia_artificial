{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoBRdgz/inteligencia_artificial/blob/main/traducci%C3%B3n_autom%C3%A1tica/traducci%C3%B3n_ingl%C3%A9s_a_espa%C3%B1ol_con_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_2xw8hdLFK1"
      },
      "source": [
        "# Traducción de Inglés a Español con Transformadores\n",
        "\n",
        "**Objetivo:**\n",
        "\n",
        "Entrenar un Transformador *seq2seq* (secuencia a secuencia) para resolver una tarea de traducción automática de inglés a español.\n",
        "\n",
        "Este problema es conocido por sus siglas en inglés como NMT (*Neural Machine Translation*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2GNvC9bLFK_"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "En este *notebook* se diseña un modelo de tipo Transformador de secuencia a secuencia utilizando la API funcional de Tensorflow, que será entrenado en una tarea de traducción automática de inglés a español.\n",
        "\n",
        "Se implementarán los siguientes puntos:\n",
        "\n",
        "- Preparar los datos para entrenar un modelo *seq2seq* donde la secuencia de entrada es una oración en inglés y la secuancia de salida será la oración traducida a español.\n",
        "- Vectorizar el texto usando la clase `TextVectorization` de Keras.\n",
        "- Implementar una clase `PositionalEmbedding`, una clase `TransformerEncoder`,\n",
        "y una clase `TransformerDecoder`.\n",
        "- Entrenar el modelo para resolver una tarea de traducción entre idiomas.\n",
        "- Utilizar el modelo para generar traducciones a oraciones que no fueron utilizadas durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('No conectado a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2KuEEw_XNWQ",
        "outputId": "66b45a34-5133-40a0-92ad-7fcf3096f93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan  4 01:20:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA7WpDtgLFLB"
      },
      "source": [
        "## Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXmG5NuNLFLC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import string\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRLSm6dHLFLE"
      },
      "source": [
        "## Conjunto de datos\n",
        "\n",
        "En el siguiente enlace se pueden encontrar diversos conjuntos de datos de oraciones en inglés traducidas a varios idiomas: https://www.manythings.org/anki/.\n",
        "\n",
        "En este caso se utilizará el conjunto de datos correspondiente a inglés - español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCvTawS4LFLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4f8d40-f6e0-4a3a-d1b7-669c9f2119c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "text_file = keras.utils.get_file(fname=\"spa-eng.zip\",\n",
        "                                 origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "                                 extract=True)\n",
        "\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW1efeD_LFLF"
      },
      "source": [
        "## Preprocesamiento de datos\n",
        "\n",
        "Cada línea del conjunto de datos contiene una oración en inglés y su correspondiente oración traducida a español.\n",
        "La oración en inglés es la secuencia de origen y la oración en español es la secuencia de destino.\n",
        "Anteponemos el *token* `\"[start]\"` y agregamos el token `\"[end]\"` a la oración en español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzPrCXHvLFLF"
      },
      "outputs": [],
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUqX5N3jLFLG"
      },
      "source": [
        "Así es como se ven los pares de oraciones (inglés, español) generados por la celda previa para 5 registros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j2HpqzSLFLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f7779a-4d5f-4178-da61-e689065f440b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Never open the door of a car that is in motion.', '[start] Nunca abras la puerta de un auto en movimiento. [end]')\n",
            "(\"Don't worry about your dog. I'll take care of him.\", '[start] No te preocupes por tu perro. Yo cuidaré de él. [end]')\n",
            "('It is rude to interrupt others.', '[start] Es grosero interrumpir a los demás. [end]')\n",
            "(\"I don't care if our team wins or not.\", '[start] No me importa si nuestro equipo gana o no. [end]')\n",
            "('Do you want to watch this movie again?', '[start] ¿Querés ver esta película otra vez? [end]')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2oBKmWtLFLH"
      },
      "source": [
        "Ahora, se particionan los pares de oraciones en los conjuntos de entrenamiento, validación, y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udGlLRQuLFLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1d556b-155b-45f6-e9e3-344c94ca0926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 pares totales\n",
            "83276 pares en el conjunto de entrenamiento\n",
            "17844 pares en el conjunto de validación\n",
            "17844 pares en el conjunto de prueba\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} pares totales (inglés, español)\")\n",
        "print(f\"{len(train_pairs)} pares en el conjunto de entrenamiento\")\n",
        "print(f\"{len(val_pairs)} pares en el conjunto de validación\")\n",
        "print(f\"{len(test_pairs)} pares en el conjunto de prueba\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-DxasW0LFLI"
      },
      "source": [
        "## Vectorización de datos de texto\n",
        "\n",
        "Se utilizan dos instancias de la clase `TextVectorization` para vectorizar los datos de texto (una para inglés y otra para español), es decir, se convierten las cadenas originales en texto a secuencias de números enteros donde cada entero representa el índice de una palabra en un vocabulario.\n",
        "\n",
        "La clase en inglés utilizará la estandarización de cadenas predeterminada (eliminar los caracteres de puntuación) y el enfoque de separación (separar por espacios en blanco), mientras que en la clase en español se utilizará una estandarización personalizada, donde se agregará el caracter `\"¿\"` al conjunto de caracteres de puntuación **a eliminar**.\n",
        "\n",
        "Importante: La limpieza de datos propuesta es sólo con fines didácticos, pues en caso de tener como objetivo el despliegue a producción de un modelo de traducción automática, no se recomendaría eliminar los caracteres de puntuación en independencia del idioma que se esté modelando. En su lugar, se recomendaría convertir cada carácter de puntuación a su propio *token*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfNvmnyWLFLJ"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ14pkFkLFLK"
      },
      "source": [
        "Ahora, se formatearán las particiones de datos.\n",
        "\n",
        "En cada paso de entrenamiento, el modelo buscará predecir las palabras objetivo (*targets*) N+1 y consecuentes usando la oración de origen (*inputs*) y las palabras objetivo 0 a N.\n",
        "\n",
        "El conjunto de datos de entrenamiento generará una tupla `(inputs, targets)`, donde:\n",
        "\n",
        "- `inputs` es un diccionario con las llaves `encoder_inputs` y `decoder_inputs`. `encoder_inputs` es la oración de origen vectorizada y `decoder_inputs` es la oración de destino \"hasta ahora\", es decir, las palabras 0 a N utilizadas para predecir la palabra N+1 (y consecuentes) en la oración de destino.\n",
        "- `target` es la oración de destino desplazada por un paso: proporciona las siguientes palabras en la oración de destino, o dicho de otra forma, lo que el modelo intentará predecir."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])"
      ],
      "metadata": {
        "id": "3CVh-L0qrPcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SQN-WDWLFLL"
      },
      "outputs": [],
      "source": [
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "3dleuFqqrTzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYJ_vLRrLFLL"
      },
      "source": [
        "Echemos un vistazo rápido a las formas de secuencia. (tenemos lotes de 64 pares y todas las secuencias tienen 20 pasos):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9h7EDnULFLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa0a871-f1a8-4143-8cdb-a8ba4e0bb91b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtcBVOkwLFLM"
      },
      "source": [
        "## Construyendo el modelo\n",
        "\n",
        "Nuestro Transformador de secuencia a secuencia consiste en un `TransformerEncoder` y un `TransformerDecoder` encadenados juntos. Para que el modelo sea consciente del orden de las palabras, también usamos una capa `PositionalEmbedding`.\n",
        "\n",
        "La secuencia fuente se pasará al `TransformerEncoder`, que producirá una nueva representación de la misma. Esta nueva representación se pasará al `TransformerDecoder`, junto con la secuencia de destino hasta el momento (palabras de destino 0 a N). El 'TransformerDecoder' intentará predecir las siguientes palabras en la secuencia de destino (N+1 y más allá).\n",
        "\n",
        "Un detalle clave que hace esto posible es el enmascaramiento causal (consulte el método `get_causal_attention_mask()` en `TransformerDecoder`). El `TransformerDecoder` ve las secuencias completas a la vez y, por lo tanto, debemos asegurarnos de que solo use información de los tokens de destino 0 a N al predecir el token N+1 (de lo contrario, podría usar información del futuro, lo que daría como resultado un modelo que no se puede utilizar en el momento de la inferencia)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "ehppqPzHUQiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "-09LYUThUUeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1a4dmhALFLM"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"latent_dim\": self.latent_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZljumWTLFLN"
      },
      "source": [
        "A continuación, ensamblamos el modelo de extremo a extremo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZxFjojaLFLN"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg0iBEjCLFLO"
      },
      "source": [
        "## Entrenando a nuestro modelo\n",
        "\n",
        "Usaremos la precisión como una forma rápida de monitorear el progreso del entrenamiento en los datos de validación. Tenga en cuenta que la traducción automática generalmente usa puntajes BLEU, así como otras métricas, en lugar de precisión.\n",
        "\n",
        "Aquí solo entrenamos durante 1 época, pero para que el modelo realmente converja, debe entrenar durante al menos 30 épocas."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se recomienda establecer al menos 30 epocas para mejores resultados\n",
        "epochs = 3\n",
        "\n",
        "ckpt = ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "transformer.compile(\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "transformer.summary()"
      ],
      "metadata": {
        "id": "cfIU5XRAsKu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJPPanK_LFLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794c875d-2a4a-4ceb-dda6-b298cade51cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n",
            " alEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
            " erEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19,960,216\n",
            "Trainable params: 19,960,216\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "1302/1302 [==============================] - 100s 70ms/step - loss: 1.6503 - accuracy: 0.4276 - val_loss: 1.2855 - val_accuracy: 0.5254\n",
            "CPU times: user 59.6 s, sys: 4.56 s, total: 1min 4s\n",
            "Wall time: 1min 39s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2aaa5d3ac0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "%%time\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[ckpt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy6ZRlc5LFLO"
      },
      "source": [
        "## Decodificación de oraciones de prueba\n",
        "\n",
        "Finalmente, demostremos cómo traducir oraciones nuevas en inglés. Simplemente ingresamos al modelo la oración en inglés vectorizada, así como el token de destino `\"[start]\"`, luego generamos repetidamente el siguiente token, hasta que llegamos al token `\"[end]\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JNN5XE6LFLO"
      },
      "outputs": [],
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de 30 épocas se obtienen los siguientes resultados:"
      ],
      "metadata": {
        "id": "TQYRHiTOYUZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, translated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQJYNLF7X_tk",
        "outputId": "c5eb0718-07ac-4ca5-b488-d36a8349eead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom jumped over the ditch. [start] tom se se se se quedó el mundo [end]\n",
            "I need to talk to you. [start] necesito hablar con ti [end]\n",
            "You have a pretty good memory. [start] tienes un buen buen buen buen buen buen buen buen buen [end]\n",
            "Where do you live? [start] dónde crees [end]\n",
            "Tom criticized Mary in front of everyone. [start] tom se se se quedó en todos en todos en todos [end]\n",
            "They are aware of the difficulties. [start] ellos son [UNK] de los otros [end]\n",
            "Tom talks very fast. [start] tom habla muy bien [end]\n",
            "She was so sad that she did not want to speak to anyone. [start] ella estaba tan feliz que ella no quería que no quería hablar con nadie [end]\n",
            "I thought you said you didn't see anything. [start] pensé que no dijo que no dijo nada [end]\n",
            "She brought his lunch today. [start] ella se se vive esta mañana [end]\n",
            "Tom had a feeling that Mary would be late. [start] tom tenía un poco que mary podría estar tarde [end]\n",
            "Can I kiss you? [start] puedo creer [end]\n",
            "The girl has a soft heart. [start] la policía tiene un paseo [end]\n",
            "He did his best to rescue her. [start] Él su vida es mejor [end]\n",
            "I guess I can't blame you after last night. [start] creo que no puedo creer que te puedo haber estado mañana [end]\n",
            "Tom is the only person who can do this work. [start] tom es el hombre que puede hacer esto para hacer esto [end]\n",
            "That old man is a fussy eater. [start] ese hombre hombre es un poco de [UNK] [end]\n",
            "He told me the truth. [start] Él me dijo la verdad [end]\n",
            "They are high school students. [start] ellos son cosas en la escuela [end]\n",
            "He is not a high school student. [start] Él no es un colegio [end]\n",
            "The ground is covered with snow. [start] el árbol está escuchando con la tienda [end]\n",
            "I hope you'll pay more attention next time. [start] espero que te [UNK] más tiempo [end]\n",
            "Six men came forward to volunteer for the work. [start] las dos niños para los cosas para hacer el trabajo [end]\n",
            "I asked Tom for Mary's phone number. [start] le preguntó a tom el profesor de lo dijo el teléfono [end]\n",
            "The treaty with Texas was signed April twelfth, 1844. [start] el árbol con el árbol se fue a los otros [end]\n",
            "Are you seriously thinking about having a baby? [start] estás en el tiempo estás solo [end]\n",
            "He's not such a great writer and I think he knows it. [start] Él no es un hombre que me gusta que me gusta que sé [end]\n",
            "Does money bring happiness? [start] el dinero se dice [end]\n",
            "Tom didn't want to go to the park. [start] tom no quería ir al colegio [end]\n",
            "Tom is beating up Mary. [start] tom está seguro de mary [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmIuAc2cWsRq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}