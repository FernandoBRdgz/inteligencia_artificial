{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoBRdgz/inteligencia_artificial/blob/main/grandes_modelos_de_lenguaje/llms_con_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción a LangChain\n",
        "\n",
        "LangChain es un marco para desarrollar aplicaciones impulsadas por modelos de lenguaje.\n",
        "\n",
        "**Objetivos** Revisión de los siguientes conceptos:\n",
        "\n",
        "- LLMs (Grandes modelos de lenguaje)\n",
        "- Prompt Templates (Plantillas)\n",
        "- Chains (Cadenas)\n",
        "- Agents and Tools (Agentes y herramientas)\n",
        "- Memory (Memoria)\n",
        "- Document Loaders (Cargadores de documentos)\n",
        "- Indexes (Índices)"
      ],
      "metadata": {
        "id": "nTDgRy0jKDkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalación"
      ],
      "metadata": {
        "id": "5WGtOYYTKfz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcrn7QRyQXGj"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. LLMs\n",
        "\n",
        "Una interfaz genérica para todos los LLM. Ver todos los proveedores de LLM: https://python.langchain.com/en/latest/modules/models/llms/integrations.html"
      ],
      "metadata": {
        "id": "NkGGSdmtta6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "H_dfy6G_aBtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "o44cR-rCEQR9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/Inteligencia Artificial/Grandes Modelos de Lenguaje'"
      ],
      "metadata": {
        "id": "aHIw1WaiDz2n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnNEMiQ8ClME",
        "outputId": "fa0968f2-ff0b-4538-fbc6-99caf40671ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(path)"
      ],
      "metadata": {
        "id": "uSE3-5XrCrcr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from config import api_key_openai"
      ],
      "metadata": {
        "id": "RBMRc2L0EYMI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = api_key_openai"
      ],
      "metadata": {
        "id": "RlxEmS1CaM5v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)  # model_name=\"text-davinci-003\"\n",
        "text = \"¿Cuál sería un buen nombre para una startup disruptiva que ofrece servicios basados en inteligencia artificial?\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "id": "pY09s9cmZ6nQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9edb7e78-4c80-4714-f3cc-18d6c3f15573"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "BrainyTech.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "idkq_aVyaceF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = api_key_hf"
      ],
      "metadata": {
        "id": "i4DKOWjyaRmO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "QmtH72oCaU32"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## https://huggingface.co/google/flan-t5-xl\n",
        "#llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
        "\n",
        "#llm(\"translate English to German: How old are you?\")"
      ],
      "metadata": {
        "id": "8uK5TtJPc49I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prompt Templates\n",
        "\n",
        "LangChain facilita la gestión y la rápida optimización.\n",
        "\n",
        "Normalmente, cuando usa un LLM en una aplicación, no está enviando la entrada del usuario directamente al LLM. En su lugar, debe tomar la entrada del usuario y construir un aviso, y solo luego enviarlo al LLM."
      ],
      "metadata": {
        "id": "3O-7dO1htdO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"¿Puede Andrés Manuel López Obrador tener una conversación con Porfirio Díaz?\")"
      ],
      "metadata": {
        "id": "_FDS9IDRapOt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1f77b0f6-0089-4a9b-a4a2-b1ffa1b12410"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nNo, esto es imposible, ya que Porfirio Díaz murió en 1915.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Question: ¿Puede Andrés Manuel López Obrador tener una conversación con Porfirio Díaz?\n",
        "\n",
        "Pensemos paso a paso.\n",
        "\n",
        "Respuesta: \"\"\"\n",
        "llm(prompt)"
      ],
      "metadata": {
        "id": "lB4W8dM1tPAY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "994e0268-e3a3-4e19-eb5f-9f7a5a05cc3f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNo, no es posible que Andrés Manuel López Obrador tenga una conversación con Porfirio Díaz, ya que Porfirio Díaz murió en 1915.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Pregunta: {pregunta}\n",
        "\n",
        "Pensemos paso a paso.\n",
        "\n",
        "Respuesta: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"pregunta\"])"
      ],
      "metadata": {
        "id": "UU1VyMMvtsCE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(pregunta=\"¿Puede Andrés Manuel López Obrador tener una conversación con Porfirio Díaz?\")"
      ],
      "metadata": {
        "id": "-Yzpc_0aHHeE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bb93a792-8800-477b-e0b6-365f87ac23f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pregunta: ¿Puede Andrés Manuel López Obrador tener una conversación con Porfirio Díaz?\\n\\nPensemos paso a paso.\\n\\nRespuesta: '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    llm(prompt)\n",
        "except Exception as e:\n",
        "    print(\"Requerimos de una cadena 'Chain'\")"
      ],
      "metadata": {
        "id": "on8ubh3kt7oD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022b11f1-ea86-48f7-9532-217c26ceb512"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requerimos de una cadena 'Chain'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Chains\n",
        "\n",
        "Combina LLM y prompts en flujos de trabajo de varios pasos."
      ],
      "metadata": {
        "id": "1zw1KlSeuUOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "question = \"¿Puede Andrés Manuel López Obrador tener una conversación con Porfirio Díaz?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "eE6n-jbAuOxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15fb36b5-a316-4a3f-a12a-9e85faad077b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No, esto no es posible dado que Porfirio Díaz fue el Presidente de México desde 1876 hasta 1911 y Andrés Manuel López Obrador es el Presidente actual de México. Por lo tanto, no hay una forma realista en que ellos puedan tener una conversación.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Agentes y Herramientas\n",
        "\n",
        "Los agentes involucran a un LLM que toma decisiones sobre qué acciones tomar, tomar esa acción, ver una observación y repetirla hasta que se termine.\n",
        "\n",
        "\n",
        "Cuando se usan correctamente, los agentes pueden ser extremadamente poderosos. Para cargar agentes, debe comprender los siguientes conceptos:\n",
        "\n",
        "- Herramienta: Una función que realiza un deber específico. Esto puede ser cosas como: búsqueda de Google, búsqueda de base de datos, Python REPL, otras cadenas.\n",
        "- LLM: El modelo de lenguaje que impulsa al agente.\n",
        "- Agente: El agente a utilizar.\n",
        "\n",
        "Herramientas: https://python.langchain.com/en/latest/modules/agents/tools.html\n",
        "\n",
        "Tipoes de Agentes: https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html"
      ],
      "metadata": {
        "id": "Zp-UlOK0bMVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent"
      ],
      "metadata": {
        "id": "79JcjhFXwv0J"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "dOSpaurEb1MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "rXsInicWIM8i"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"
      ],
      "metadata": {
        "id": "RgV4kny1bgy1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "iQUOsWLrbjKv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"¿En qué año se estrenó la película Inception con Leonardo Dicaprio? ¿Cuál sería este año elevado a la potencia 0,43?\")"
      ],
      "metadata": {
        "id": "M8Rob2Wsb_l9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "b50db39a-3a8d-47eb-f726-a7112a1e2bf6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out the year the movie was released and then use the calculator to calculate the power.\n",
            "Action: Wikipedia\n",
            "Action Input: Inception (2010 film)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: Inception\n",
            "Summary: Inception is a 2010 science fiction action film written and directed by Christopher Nolan, who also produced the film with Emma Thomas, his wife. The film stars Leonardo DiCaprio as a professional thief who steals information by infiltrating the subconscious of his targets. He is offered a chance to have his criminal history erased as payment for the implantation of another person's idea into a target's subconscious. The ensemble cast includes Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Elliot Page, Tom Hardy, Dileep Rao, Cillian Murphy, Tom Berenger, and Michael Caine.\n",
            "After the 2002 completion of Insomnia, Nolan presented to Warner Bros. a written 80-page treatment for a horror film envisioning \"dream stealers,\" based on lucid dreaming. Deciding he needed more experience before tackling a production of this magnitude and complexity, Nolan shelved the project and instead worked on 2005's Batman Begins, 2006's The Prestige, and The Dark Knight in 2008. The treatment was revised over 6 months and was purchased by Warner in February 2009. Inception was filmed in six countries, beginning in Tokyo on June 19 and ending in Canada on November 22. Its official budget was $160 million, split between Warner Bros. and Legendary. Nolan's reputation and success with The Dark Knight helped secure the film's US$100 million in advertising expenditure.\n",
            "Inception's premiere was held in London on July 8, 2010; it was released in both conventional and IMAX theaters beginning on July 16, 2010. Inception grossed over $828 million worldwide, becoming the fourth-highest-grossing film of 2010. Considered one of the best films of the 2010s, Inception won four Academy Awards (Best Cinematography, Best Sound Editing, Best Sound Mixing, and Best Visual Effects) and was nominated for four more: Best Picture, Best Original Screenplay, Best Art Direction, and Best Original Score.\n",
            "\n",
            "Page: Inception (soundtrack)\n",
            "Summary: Inception: Music from the Motion Picture is the soundtrack to the 2010 film of the same name directed by Christopher Nolan, released under Reprise Records on July 13, 2010. Hans Zimmer scored the film, marking his third collaboration with Nolan following Batman Begins and The Dark Knight.\n",
            "\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the year the movie was released.\n",
            "Action: Calculator\n",
            "Action Input: 2010^0.43\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 26.3253591394933\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: El año 2010 elevado a la potencia 0,43 es 26.3253591394933.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'El año 2010 elevado a la potencia 0,43 es 26.3253591394933.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Memoria\n",
        "\n",
        "Agregar Estado a Cadenas y Agentes.\n",
        "\n",
        "La memoria es el concepto de estado persistente entre llamadas de una cadena/agente. LangChain proporciona una interfaz estándar para la memoria, una colección de implementaciones de memoria y ejemplos de cadenas/agentes que usan memoria."
      ],
      "metadata": {
        "id": "8AuQNfhYm48A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain"
      ],
      "metadata": {
        "id": "tPaPubGMKZpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "conversation.predict(input=\"Hola!\")"
      ],
      "metadata": {
        "id": "Ujwj29G2cDPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "82de0ac2-2060-432b-b53d-88d464798240"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hola!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hola! ¿Cómo estás? Me llamo AI. ¿Cómo te llamas?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"¿Podemos hablar de IA?\")"
      ],
      "metadata": {
        "id": "XkKv8n7ZnB2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "8649fa2a-f150-4913-8c11-7882ea342359"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hola!\n",
            "AI:  Hola! ¿Cómo estás? Me llamo AI. ¿Cómo te llamas?\n",
            "Human: ¿Podemos hablar de IA?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' ¡Claro! ¿Qué quieres saber sobre Inteligencia Artificial?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Estoy interesado en la inteligencia artificial general\")"
      ],
      "metadata": {
        "id": "r4P3zWCmoDST",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "671fe4f2-46d6-4922-a33e-c1820a9c04c9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hola!\n",
            "AI:  Hola! ¿Cómo estás? Me llamo AI. ¿Cómo te llamas?\n",
            "Human: ¿Podemos hablar de IA?\n",
            "AI:  ¡Claro! ¿Qué quieres saber sobre Inteligencia Artificial?\n",
            "Human: Estoy interesado en la inteligencia artificial general\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' La Inteligencia Artificial General se refiere a la creación de sistemas informáticos que pueden realizar tareas inteligentes sin necesidad de programación específica. Estos sistemas pueden aprender, planificar, razonar, entender el lenguaje y tomar decisiones.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Cargadores de documentos\n",
        "\n",
        "La combinación de modelos de lenguaje con sus propios datos de texto es una forma poderosa de diferenciarlos. El primer paso para hacer esto es cargar los datos en \"documentos\", una forma elegante de decir algunos fragmentos de texto. Este módulo tiene como objetivo hacer esto fácil.\n",
        "\n",
        "https://python.langchain.com/en/latest/modules/indexes/document_loaders.html"
      ],
      "metadata": {
        "id": "9wMttXM-CuPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import NotionDirectoryLoader"
      ],
      "metadata": {
        "id": "dIcXQn1_Kdy2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = NotionDirectoryLoader(\"Notion_DB\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "iAiISOcboPKR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Índices\n",
        "\n",
        "Los índices se refieren a formas de estructurar documentos para que los LLM puedan interactuar mejor con ellos. Este módulo contiene funciones de utilidad para trabajar con documentos.\n",
        "\n",
        "- Incrustaciones: Una incrustación es una representación numérica de una información, por ejemplo, texto, documentos, imágenes, audio, etc.\n",
        "- Divisores de texto: Cuando desee manejar fragmentos largos de texto, es necesario dividir ese texto en fragmentos.\n",
        "- Almacenes de vectores: Las bases de datos vectoriales almacenan e indexan incrustaciones de vectores de modelos NLP para comprender el significado y el contexto de cadenas de texto, oraciones y documentos completos para obtener resultados de búsqueda más precisos y relevantes."
      ],
      "metadata": {
        "id": "Cwlwe_iWKucO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
        "    f.write(res.text)"
      ],
      "metadata": {
        "id": "qLU79cyCozYl"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader"
      ],
      "metadata": {
        "id": "KkSlw3uuLA9V"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "XGyZXiJZBsov"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "gqLFKKHULDJH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "OklI0xTvp2KE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "skvXSMXHCxyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "JExaota0LO_B"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "#text = \"This is a test document.\"\n",
        "#query_result = embeddings.embed_query(text)\n",
        "#doc_result = embeddings.embed_documents([text])"
      ],
      "metadata": {
        "id": "V1yCdAhSCi64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "8R3pT55b-uBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Almacen de vectores: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "GOaY0V9bLfOY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = db.similarity_search(query)"
      ],
      "metadata": {
        "id": "W7sRydnlC7rb"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB7lvDWzDHZy",
        "outputId": "5d8794b8-929a-4a31-cf6e-a158fbaba9a6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
            "\n",
            "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
            "\n",
            "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
            "\n",
            "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.save_local(\"faiss_index\")\n",
        "new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
        "docs = new_db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "nu-AmhDLEK0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80bc4e26-f90e-45dd-bd45-45ef3d23998a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
            "\n",
            "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
            "\n",
            "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
            "\n",
            "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Por hacer**\n",
        "\n",
        "* Contrastar ejemplos con modelo flan-t5-xl\n",
        "* Profundizar en el tema de Cargadores de Documentos\n",
        "* Probar el último ejemplo con alguna lectura en español"
      ],
      "metadata": {
        "id": "eaHhp5etL0ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referencias**\n",
        "\n",
        "* https://github.com/hwchase17/langchain\n",
        "* https://python.langchain.com/en/latest/index.html\n"
      ],
      "metadata": {
        "id": "7oSLo6kU6z0G"
      }
    }
  ]
}