{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1J51QjaYw-zlwRXep9xaL1x1Xwvq8u5yg",
      "authorship_tag": "ABX9TyOyce5A6Riqf4R0h4CHB9+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoBRdgz/inteligencia_artificial/blob/main/q%26a/langchain_cohere.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QkSDNEIGYiQN"
      },
      "outputs": [],
      "source": [
        "!pip install cohere langchain==0.0.91 chromadb==0.3.2 tfds-nightly> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "from langchain.embeddings.cohere import CohereEmbeddings\n",
        "from langchain.llms import Cohere\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "RQihIkd8Ynw0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "d1WZQH68ZTdA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos el discurso de graduación de la Universidad de Stanford de Steve Jobs como ejemplo.\n",
        "# Enlace: https://news.stanford.edu/2005/06/12/youve-got-find-love-jobs-says/\n",
        "\n",
        "!wget 'https://docs.google.com/uc?export=download&id=1f1INWOfJrHTFmbyF_0be5b4u_moz3a4F' -O steve-jobs-commencement.txt"
      ],
      "metadata": {
        "id": "fiURawZvaeSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('steve-jobs-commencement.txt') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "LwMyDRhvatoe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
        "texts = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "extlZDpGa7ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = CohereEmbeddings(model = \"multilingual-22-12\")\n",
        "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": f\"{i}\"} for i in range(len(texts))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O83chVQra76j",
        "outputId": "fd1573ee-9380-48aa-d489-32b30ba248f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Chroma using direct local API.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:Logger created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using DuckDB in-memory for database. Data will be transient.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:Index not found\n",
            "DEBUG:Chroma:Index saved to {self._save_folder}/index.bin\n",
            "DEBUG:Chroma:Index saved to {self._save_folder}/index.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "           \"What did the author liken The Whole Earth Catalog to?\",\n",
        "           \"What was Reed College great at?\",\n",
        "           \"What was the author diagnosed with?\",\n",
        "           \"What is the key lesson from this article?\",\n",
        "           \"What did the article say about Michael Jackson?\",\n",
        "           ]"
      ],
      "metadata": {
        "id": "7BayOtX3bAVR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Text: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer the question based on the text provided. If the text doesn't contain the answer, reply that the answer is not available.\"\"\""
      ],
      "metadata": {
        "id": "hIIZSnXgbDBV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "90LeZ_NzbLHy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar la respuesta dado el contexto\n",
        "chain = load_qa_chain(Cohere(model=\"command-xlarge-nightly\", temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "for question in questions:\n",
        "    docs = docsearch.similarity_search(question)\n",
        "    answer = chain.run(input_documents=docs, question=question)\n",
        "    answer = answer.replace(\"\\n\",\"\").replace(\"Answer:\",\"\")\n",
        "    print(\"-\"*20,\"\\n\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evFhRu0obL1r",
        "outputId": "c4352af9-d39f-4fa0-ba26-773ec8269eba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 3.5762786865234375e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.00022602081298828125\n",
            "DEBUG:Chroma:time to pre process our knn query: 4.291534423828125e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0001995563507080078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- \n",
            "\n",
            "Question: What did the author liken The Whole Earth Catalog to?\n",
            "Answer: Google\n",
            "-------------------- \n",
            "\n",
            "Question: What was Reed College great at?\n",
            "Answer: Question: What was Reed College great at? Reed College at that time offered perhaps the best calligraphy instruction in the country.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 4.76837158203125e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0003330707550048828\n",
            "DEBUG:Chroma:time to pre process our knn query: 4.0531158447265625e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.00011706352233886719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- \n",
            "\n",
            "Question: What was the author diagnosed with?\n",
            "Answer: Cancer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 5.4836273193359375e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0022115707397460938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- \n",
            "\n",
            "Question: What is the key lesson from this article?\n",
            "Answer: Stay Hungry. Stay Foolish.\n",
            "-------------------- \n",
            "\n",
            "Question: What did the article say about Michael Jackson?\n",
            "Answer: Stay Hungry. Stay Foolish.Michael Jackson died.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referencias**\n",
        "\n",
        "* https://cohere.ai/\n",
        "* https://python.langchain.com/en/latest/getting_started/getting_started.html"
      ],
      "metadata": {
        "id": "esYJzYywb0Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Por hacer**\n",
        "\n",
        "* Añadir descripción del modelo\n",
        "* Añadir comentarios\n",
        "* Adaptar a ejemplo en español"
      ],
      "metadata": {
        "id": "q_BUGsZgcA3n"
      }
    }
  ]
}