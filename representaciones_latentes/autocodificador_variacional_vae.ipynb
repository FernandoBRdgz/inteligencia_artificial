{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs41IuKpwzEWdQYpw0/Gyc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoBRdgz/inteligencia_artificial/blob/main/representaciones_latentes/autocodificador_variacional_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducción\n",
        "\n",
        "Este *notebook* muestra como construir un autocodificador variacional, mejor conocido como Variational Autoencoder (VAE) entrenando un modelo en el conjunto de datos MNIST para generar nuevas imágenes.\n",
        "\n",
        "Un autocodificador variacional es un modelo generativo basado en probabilidad. Consiste en un codificador, que toma datos x como entrada y los transforma en una representación latente z, y un decodificador, que toma una representación latente z y devuelve una reconstrucción y. La inferencia se realiza a través de la inferencia variacional para aproximar la parte posterior del modelo."
      ],
      "metadata": {
        "id": "H3X_nrDZfHST"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RC4jB-5Je91Y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "latent_dim = 2"
      ],
      "metadata": {
        "id": "0udU8tWrgNie"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_image(image, label):\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    image /= 255.0\n",
        "    image = tf.reshape(image, shape=(28,28,1,))\n",
        "    return image"
      ],
      "metadata": {
        "id": "rQal3vwbUmTG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(map_fn, is_validation=False):\n",
        "    if is_validation:\n",
        "        split_name = \"test\"\n",
        "    else:\n",
        "        split_name = \"train\"\n",
        "  \n",
        "    dataset = tfds.load(\"mnist\", as_supervised=True, split=split_name)\n",
        "    dataset = dataset.map(map_fn)\n",
        "\n",
        "    if is_validation:\n",
        "        dataset = dataset.batch(batch_size)\n",
        "    else:\n",
        "        dataset = dataset.shuffle(1024).batch(batch_size)\n",
        "  \n",
        "    return dataset"
      ],
      "metadata": {
        "id": "hsnMnFIbgVGE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = get_dataset(map_image, is_validation=False)\n",
        "val_dataset = get_dataset(map_image, is_validation=True)"
      ],
      "metadata": {
        "id": "WerA8xGTgXwC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensiones del primer lote\n",
        "list(train_dataset.as_numpy_iterator())[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdZFLry1XJWJ",
        "outputId": "6bc6d458-8ea5-4765-af82-f53f691cb825"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Imagen del primer lote\n",
        " plt.imshow(list(train_dataset.as_numpy_iterator())[0][0], cmap=\"gray\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "WhQWMqg-Zw9_",
        "outputId": "0604ce95-3bc8-4586-888b-598b9391f85a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOiElEQVR4nO3dXYxUdZrH8d+zvHgBcwG+EAR2nRmMybi6sCGERFwlE9D1JYAGHGJW151sewE6xE2UuCrEdYyuzOrGC2JPUGDDSogI0040DEsm9syFxtYg4gvTvQYEAvS6GBVj5MVnL+qwabDrX+05deoUPN9P0umq89Q550nRP86p+tepv7m7AJz7/qzqBgC0BmEHgiDsQBCEHQiCsANBDG/lzsyMt/6Bkrm7Dba80JHdzK43s91m1mdmy4psC0C5LO84u5kNk/QnSbMl7Zf0lqRF7v5BYh2O7EDJyjiyT5fU5+4fu/sxSRskzS2wPQAlKhL2CZL2Dbi/P1t2GjPrMLMeM+spsC8ABZX+Bp27d0rqlDiNB6pU5Mh+QNKkAfcnZssAtKEiYX9L0qVm9kMzGynpZ5K6mtMWgGbLfRrv7ifMbImkrZKGSXre3d9vWmcAmir30FuunfGaHShdKR+qAXD2IOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCI3FM24+wwYsSIZH3Lli3J+qpVq5L1WbNmJev33Xdfsl7Evn37kvU5c+bUrfX29ibXPXnyZK6e2lmhsJvZHklfSjop6YS7T2tGUwCarxlH9lnu/mkTtgOgRLxmB4IoGnaX9Dsze9vMOgZ7gJl1mFmPmfUU3BeAAoqexs909wNmdpGkbWb2kbt3D3yAu3dK6pQkM/OC+wOQU6Eju7sfyH73S9osaXozmgLQfLnDbmajzOwHp25LmiNpV7MaA9Bc5p7vzNrMfqTa0VyqvRz4T3f/ZYN1OI1vsbvuuitZX716daHtm1mynvfvq2wrVqxI1h9//PFk/cSJE03sprncfdB/lNyv2d39Y0l/lbsjAC3F0BsQBGEHgiDsQBCEHQiCsANB5B56y7Uzht5abtOmTcn6/PnzC22/0dBbf39/3dr69euT606ePDlZnzp1arI+YcKEZD1l+vT058N6etr309/1ht44sgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyznwNmzJhRt7Zt27bkuqNGjSq072+++SZZv+KKK+rW+vr6Cu37wgsvTNY3btxYt3bNNdck1+3q6krWFyxYkKwfP348WS8T4+xAcIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7OeA1DXrRa9XbzQt8kcffZSsX3fddYX2X0SZnz+4+OKLk/VDhw4l62VinB0IjrADQRB2IAjCDgRB2IEgCDsQBGEHgsg9iyta5/bbb0/W58yZk3vbe/fuTdZnz56drBe9Jr1Mb7zxRt3a1q1bk+vecsstzW6ncg2P7Gb2vJn1m9muAcvGmtk2M+vNfo8pt00ARQ3lNH6NpOvPWLZM0nZ3v1TS9uw+gDbWMOzu3i3pyBmL50pam91eK2lec9sC0Gx5X7OPc/eD2e1DksbVe6CZdUjqyLkfAE1S+A06d/fUBS7u3impU+JCGKBKeYfeDpvZeEnKftefqhNAW8gb9i5Jd2a375T0m+a0A6AsDa9nN7MXJV0r6QJJhyUtl7RF0kZJfy5pr6SF7n7mm3iDbYvT+EE0+v7z7u7uZP2yyy7Lve8pU6Yk6zt37sy97Xa2cOHCZH3Dhg3J+tl4PXvD1+zuvqhO6aeFOgLQUnxcFgiCsANBEHYgCMIOBEHYgSC4xLUFzj///GT9pZdeStaLDK319vYm640ucT1XpaaSPldxZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnb4FGX0t89dVXl7bvZ555Jln//PPPS9t3O7vpppuqbqHlOLIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMsw/R6NGj69aWL1+eXHfp0qVN7uZ0mzdvrlt77rnnSt13O0tNZc317ADOWYQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7EN09OjRurVdu3Yl1x0+vNyn2WzQGXrDe/jhh+vWhg0bllz3xIkTyXqjqc7bUcMju5k9b2b9ZrZrwLIVZnbAzHZkPzeU2yaAooZyGr9G0vWDLH/a3adkP682ty0AzdYw7O7eLelIC3oBUKIib9AtMbOd2Wn+mHoPMrMOM+sxs54C+wJQUN6wr5L0Y0lTJB2U9Kt6D3T3Tnef5u7Tcu4LQBPkCru7H3b3k+7+raRfS5re3LYANFuusJvZ+AF350tKjz0BqFzDAWAze1HStZIuMLP9kpZLutbMpkhySXsk3V1ei+3v3XffTdaLjskeOZJ+fzT1Heh3353+p1m1alWuntrByJEjk/URI0bUrTX6N1m3bl2yfvjw4WS9HTUMu7svGmTx6hJ6AVAiPi4LBEHYgSAIOxAEYQeCIOxAEFzi2gT33ntvqdufN29esn7VVVfVrS1ZsiS57muvvZas79mzJ1kv03nnnZesr1y5MlmfPr3+Z71SlyxL0pYtW5L1sxFHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgnH2NvDVV18l643GhJ988snc+16wYEGy/tRTT+XedlGzZs1K1hcvXpx722vWrEnWX3nlldzbblcc2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZ28CxY8eS9ePHj+fedpEx+LLNmDEjWX/hhRcKbX/z5s11aw899FChbZ+NOLIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMs7eBMWPGFKpXafjw9J/Q/Pnz69aeffbZ5LoXXXRRsv7ZZ58l64888kjd2hdffJFc91zU8MhuZpPM7Pdm9oGZvW9mv8iWjzWzbWbWm/1u379IAEM6jT8h6Z/c/SeSZkhabGY/kbRM0nZ3v1TS9uw+gDbVMOzuftDd38lufynpQ0kTJM2VtDZ72FpJ80rqEUATfK/X7GZ2iaSpkt6UNM7dD2alQ5LG1VmnQ1JHgR4BNMGQ3403s9GSNkla6u6nvbvh7i7JB1vP3TvdfZq7TyvUKYBChhR2MxuhWtDXu/vL2eLDZjY+q4+X1F9OiwCawWoH5cQDzEy11+RH3H3pgOVPSfpfd3/CzJZJGuvu9zfYVnpnZ6mZM2cm693d3YW2v3Xr1mQ9dSnoq6++mly30ddUN3LHHXck642+sjll9+7dyfrNN9+crPf19eXe99nM3W2w5UN5zX6VpL+T9J6Z7ciWPSjpCUkbzeznkvZKWtiEPgGUpGHY3f2Pkgb9n0LST5vbDoCy8HFZIAjCDgRB2IEgCDsQBGEHgmg4zt7UnZ2j4+wjR45M1h999NFk/f77kx9PKOSTTz5J1ot8TbUkTZ48OVlP/X11dXUl173nnnuS9X379iXrUdUbZ+fIDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM7eAhMnTkzWn3766WT91ltvbWY7TVX7uoP6Xn/99bq1Bx54ILnum2++maun6BhnB4Ij7EAQhB0IgrADQRB2IAjCDgRB2IEgGGdvA8OGDUvWL7/88mT9tttuq1u78sork+veeOONyfrKlSuT9a+//jpZf+yxx+rWil5Lj8Exzg4ER9iBIAg7EARhB4Ig7EAQhB0IgrADQQxlfvZJktZJGifJJXW6+7+b2QpJ/yjpf7KHPujuycnAGWcHyldvnH0oYR8vaby7v2NmP5D0tqR5qs3HftTd05+6OH1bhB0oWb2wD2V+9oOSDma3vzSzDyVNaG57AMr2vV6zm9klkqZKOvV9QUvMbKeZPW9mY+qs02FmPWbWU6xVAEUM+bPxZjZa0uuSfunuL5vZOEmfqvY6/l9UO9X/hwbb4DQeKFnu1+ySZGYjJP1W0lZ3/7dB6pdI+q27/2WD7RB2oGS5L4Sx2teHrpb04cCgZ2/cnTJf0q6iTQIoz1DejZ8p6Q+S3pP0bbb4QUmLJE1R7TR+j6S7szfzUtviyA6UrNBpfLMQdqB8XM8OBEfYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IouEXTjbZp5L2Drh/QbasHbVrb+3al0RveTWzt7+oV2jp9ezf2blZj7tPq6yBhHbtrV37kugtr1b1xmk8EARhB4KoOuydFe8/pV17a9e+JHrLqyW9VfqaHUDrVH1kB9AihB0IopKwm9n1ZrbbzPrMbFkVPdRjZnvM7D0z21H1/HTZHHr9ZrZrwLKxZrbNzHqz34POsVdRbyvM7ED23O0wsxsq6m2Smf3ezD4ws/fN7BfZ8kqfu0RfLXneWv6a3cyGSfqTpNmS9kt6S9Iid/+gpY3UYWZ7JE1z98o/gGFmfyPpqKR1p6bWMrN/lXTE3Z/I/qMc4+4PtElvK/Q9p/Euqbd604z/vSp87po5/XkeVRzZp0vqc/eP3f2YpA2S5lbQR9tz925JR85YPFfS2uz2WtX+WFquTm9twd0Puvs72e0vJZ2aZrzS5y7RV0tUEfYJkvYNuL9f7TXfu0v6nZm9bWYdVTcziHEDptk6JGlclc0MouE03q10xjTjbfPc5Zn+vCjeoPuume7+15L+VtLi7HS1LXntNVg7jZ2ukvRj1eYAPCjpV1U2k00zvknSUnf/YmCtyudukL5a8rxVEfYDkiYNuD8xW9YW3P1A9rtf0mbVXna0k8OnZtDNfvdX3M//c/fD7n7S3b+V9GtV+Nxl04xvkrTe3V/OFlf+3A3WV6uetyrC/pakS83sh2Y2UtLPJHVV0Md3mNmo7I0TmdkoSXPUflNRd0m6M7t9p6TfVNjLadplGu9604yr4ueu8unP3b3lP5JuUO0d+f+W9M9V9FCnrx9Jejf7eb/q3iS9qNpp3XHV3tv4uaTzJW2X1CvpvySNbaPe/kO1qb13qhas8RX1NlO1U/SdknZkPzdU/dwl+mrJ88bHZYEgeIMOCIKwA0EQdiAIwg4EQdiBIAg7EARhB4L4P66FolJ2IlcpAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Codificador"
      ],
      "metadata": {
        "id": "NZI1wRdTWnKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        mu, sigma = inputs\n",
        "\n",
        "        batch = tf.shape(mu)[0]\n",
        "        dim = tf.shape(mu)[1]\n",
        "\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "\n",
        "        return mu + tf.exp(0.5 * sigma) * epsilon"
      ],
      "metadata": {
        "id": "vTQaVFChgZ1H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layers(inputs, latent_dim):\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\", name=\"encode_conv1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\", name=\"encoder_conv2\")(x)\n",
        "\n",
        "    batch_2 = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_2)\n",
        "    x = tf.keras.layers.Dense(20, activation=\"relu\", name=\"encoder_dense\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    mu = tf.keras.layers.Dense(latent_dim, name=\"latent_mu\")(x)\n",
        "    sigma = tf.keras.layers.Dense(latent_dim, name=\"latent_sigma\")(x)\n",
        "\n",
        "    return mu, sigma, batch_2.shape"
      ],
      "metadata": {
        "id": "Zeub40m7gqWp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_model(latent_dim, input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=latent_dim)\n",
        "    z = Sampling()((mu, sigma))\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "    return model, conv_shape"
      ],
      "metadata": {
        "id": "umFqRJO0g1Jb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decodificador"
      ],
      "metadata": {
        "id": "etIgq2ryWpfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_layers(inputs, conv_shape):\n",
        "    units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "    x = tf.keras.layers.Dense(units, activation=\"relu\", name=\"decode_dense1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\", name=\"decode_conv2d_2\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\", name=\"decode_conv2d_3\")(x)\n",
        "    x =tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding=\"same\", activation=\"sigmoid\", name=\"decode_final\")(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "fcJdxv2mWrWS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_model(latent_dim, conv_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "    outputs = decoder_layers(inputs, conv_shape)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "dr07QXdsW7vV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Por hacer**\n",
        "\n",
        "* Descripción del modelo VAE\n",
        "* Función de costo\n",
        "* Entrenamiento de modelo\n",
        "* Predicciones\n",
        "* Comentar funciones\n",
        "* Referencia\n"
      ],
      "metadata": {
        "id": "uY_ErMhKhBQc"
      }
    }
  ]
}