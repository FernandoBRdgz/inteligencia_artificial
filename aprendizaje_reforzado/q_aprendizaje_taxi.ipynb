{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlKuR3/T+yWJZJHjGQcSI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoBRdgz/inteligencia_artificial/blob/main/aprendizaje_reforzado/q_aprendizaje_taxi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo de diseño: Taxi auto-conducido\n",
        "\n",
        "En este ejemplo se va a diseñar una simulación de un taxi auto-conducido. El objetivo principal es demostrar, en un entorno simplificado, cómo se pueden utilizar las técnicas de RL para desarrollar un enfoque eficiente y seguro para abordar el problema.\n",
        "\n",
        "El trabajo del SmartCab es recoger al pasajero en un lugar y dejarlo en otro. Estas son algunas de las cosas de las que nos gustaría que nuestro smartcab se encargara:\n",
        "\n",
        "- Dejar al pasajero en el lugar correcto\n",
        "- Ahorrar tiempo al pasajero tardando lo mínimo posible en dejarle.\n",
        "- Cuidar la seguridad del pasajero y las normas de tráfico\n",
        "\n",
        "Hay diferentes aspectos que hay que considerar aquí mientras se modela una solución RL para este problema: recompensas, estados y acciones."
      ],
      "metadata": {
        "id": "iOWo2vfLYSPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 Recompensas\n",
        "\n",
        "Dado que el agente (el conductor imaginario) está motivado por la recompensa y va a aprender a controlar el taxi mediante experiencias de prueba de entorno, tenemos que decidir las recompensas y/o penalizaciones y su magnitud en consecuencia. He aquí algunos puntos a tener en cuenta:\n",
        "\n",
        "- El agente debe recibir una alta recompensa positiva por una entrega exitosa porque este comportamiento es muy deseado\n",
        "- El agente debe ser penalizado si intenta dejar a un pasajero en lugares equivocados\n",
        "- El agente debería recibir una ligera recompensa negativa por no llegar al destino después de cada paso de tiempo. \"Ligeramente\" negativo porque preferimos que nuestro agente llegue tarde en lugar de hacer movimientos erróneos tratando de llegar al destino lo más rápido posible."
      ],
      "metadata": {
        "id": "QwY24h7sbe8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Espacio de estados\n",
        "\n",
        "En el aprendizaje por refuerzo, el agente se encuentra con un estado, y entonces toma una acción de acuerdo con el estado en el que se encuentra.\n",
        "\n",
        "El espacio de estados es el conjunto de todas las situaciones posibles en las que puede encontrarse nuestro taxi. El estado debe contener información útil que el agente necesita para realizar la acción correcta.\n",
        "\n",
        "Digamos que tenemos un área de entrenamiento para nuestro SmartCab en el que le estamos enseñando a transportar personas en un aparcamiento a cuatro lugares diferentes (R,G,Y,B):"
      ],
      "metadata": {
        "id": "fhHY3unAeEjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png'>"
      ],
      "metadata": {
        "id": "kUEWDHr3ftBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supongamos que Smartcab es el único vehículo en este estacionamiento. Podemos dividir el estacionamiento en una cuadrícula de 5x5, lo que nos brinda 25 posibles ubicaciones de taxis. Estos 25 lugares son una parte de nuestro espacio de estados. Observe que el estado de ubicación actual de nuestro taxi es la coordenada (3, 1).\n",
        "\n",
        "También notará que hay cuatro (4) ubicaciones en las que podemos recoger y dejar a un pasajero: R, G, Y, B o `[(0,0), (0,4), (4,0), (4,3)]` en (fila, columna) coordenadas. Nuestro pasajero ilustrado está en la ubicación Y y desea ir a la ubicación R.\n",
        "\n",
        "Cuando también contabilizamos un (1) estado de pasajero adicional dentro del taxi, podemos tomar todas las combinaciones de ubicaciones de pasajeros y ubicaciones de destino para llegar a un número total de estados para nuestro entorno de taxi; hay cuatro (4) destinos y cinco (4 + 1) ubicaciones de pasajeros.\n",
        "\n",
        "Por lo tanto, nuestro entorno de taxi tiene un total de 5x5x5x4=500 estados posibles."
      ],
      "metadata": {
        "id": "YvjbY7JMgwpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Espacio de acción\n",
        "\n",
        "El agente se encuentra con uno de los 500 estados y realiza una acción. La acción en nuestro caso puede ser moverse en una dirección o decidir recoger/dejar a un pasajero.\n",
        "\n",
        "En otras palabras, tenemos seis acciones posibles:\n",
        "\n",
        "1. Sur\n",
        "2. Norte\n",
        "3. Este\n",
        "4. Oeste\n",
        "5. Recoger un pasajero\n",
        "6. Dejar al pasajero\n",
        "\n",
        "\n",
        "Este es el espacio de acción : el conjunto de todas las acciones que nuestro agente puede realizar en un estado dado.\n",
        "\n",
        "Notarás en la ilustración de arriba que el taxi no puede realizar ciertas acciones en ciertos estados debido a las paredes. En el código del entorno, simplemente proporcionaremos una penalización de -1 por cada pared golpeada y el taxi no se moverá a ningún lado. Esto solo acumulará multas que harán que el taxi considere dar la vuelta a la pared."
      ],
      "metadata": {
        "id": "DgOAHkT0i3RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementación con Python\n",
        "\n",
        "Afortunadamente, OpenAI Gym tiene este entorno exacto ya creado para nosotros.\n",
        "\n",
        "Gym proporciona diferentes entornos de juego que podemos conectar a nuestro código y probar un agente. La biblioteca se encarga de la API para proporcionar toda la información que nuestro agente requiera, como posibles acciones, puntuación y estado actual. Solo tenemos que centrarnos en la parte del algoritmo para nuestro agente.\n",
        "\n",
        "Usaremos el entorno Gym llamado Taxi-V2, del cual se extrajeron todos los detalles explicados anteriormente. Los objetivos, recompensas y acciones son todos iguales."
      ],
      "metadata": {
        "id": "_Pqugl4Xjk8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necesitamos instalar gymprimero. Ejecutar lo siguiente en un cuaderno Jupyter debería funcionar:"
      ],
      "metadata": {
        "id": "yNmriCbKkG__"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou0PQnvjYBg-",
        "outputId": "aa3dbcbb-a6c8-4736-9a70-949ddcecb845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (3.22.6)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (2.2.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (6.0.0)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.10.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.11.0)\n"
          ]
        }
      ],
      "source": [
        "# Para ejecuciones fuera del entorno de Google Colab, ejecutar:\n",
        "!pip install cmake 'gym[atari]' scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[toy_text]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrEItdtxuUqB",
        "outputId": "9c7a248c-0017-4305-9e17-aaf66823146a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[toy_text] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[toy_text]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[toy_text]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[toy_text]) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[toy_text]) (2.2.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (3.11.0)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez instalado, podemos cargar el entorno del juego y renderizar lo que parece:"
      ],
      "metadata": {
        "id": "t02ccSEUkJML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "metadata": {
        "id": "9RQX6ksgYQ7G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v3\").env\n",
        "disable_render_order_enforcing=True\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "FRrwwbWQYQ9Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "# for i in range(0,100):\n",
        "#     clear_output(wait=True)\n",
        "#     env.reset()\n",
        "#     env.render()\n",
        "#     time.sleep(0.5)"
      ],
      "metadata": {
        "id": "EMbr_RKlu_8j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.s = 328\n",
        "env.render()\n",
        "print(env.step(2))\n",
        "time.sleep(10)\n",
        "clear_output(wait=True)\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq1qgCtDvBZu",
        "outputId": "fb2b1ff3-8c7c-4222-bb69-c45a4f800566"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(407, -1, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/karthikcs1/reinforcement-learning-taxi-v3-openai/notebook"
      ],
      "metadata": {
        "id": "eSpOXMUkvSp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La interfaz núcleo de la librería gym es `env`, que es la interfaz de entorno unificado. Los siguientes son los métodos `env` que nos serían muy útiles:\n",
        "\n",
        "- `env.reset`: reinicia el entorno y devuelve un estado inicial aleatorio.\n",
        "- `env.step(action)`: Paso el medio ambiente por un paso de tiempo. Devoluciones\n",
        "\n",
        "* observation : Observaciones del entorno\n",
        "* reward : si tu acción fue beneficiosa o no\n",
        "* done : Indica si hemos recogido y dejado con éxito a un pasajero, también llamado episodio único\n",
        "* info : información adicional como el rendimiento y la latencia con fines de depuración\n",
        "\n",
        "- `env.render`: Representa un cuadro del entorno (útil para visualizar el entorno)\n",
        "\n",
        "\n",
        "Nota: estamos usando al `.env` final de `make` para evitar que el entrenamiento se detenga en 200 iteraciones, que es el valor predeterminado para la nueva versión de Gym ( referencia )."
      ],
      "metadata": {
        "id": "7MOgyry5mSI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recordatorio de nuestro problema\n",
        "\n",
        "Aquí está nuestra declaración de problema reestructurada (de Gym docs):\n",
        "\n",
        "\"Hay 4 ubicaciones (etiquetadas con letras diferentes), y nuestro trabajo es recoger al pasajero en una ubicación y dejarlo en otra. Recibimos +20 puntos por una entrega exitosa y perdemos 1 punto por cada vez- paso que toma. También hay una penalización de 10 puntos por acciones ilegales de recogida y entrega\".\n",
        "\n",
        "Vamos a sumergirnos más en el medio ambiente."
      ],
      "metadata": {
        "id": "RjER8KBzoPcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBPWyICwYQ_e",
        "outputId": "43ddafdf-894b-45ab-b6c2-6f5ec4d5a80b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- El cuadrado relleno representa el taxi, que es amarillo sin pasajero y verde con pasajero.\n",
        "- La tubería (\"|\") representa una pared que el taxi no puede cruzar.\n",
        "- R, G, Y, B son las posibles ubicaciones de recogida y destino. La letra azul representa el lugar actual de recogida de pasajeros y la letra morada es el destino actual.\n",
        "\n",
        "Como se verifica en las impresiones, tenemos un espacio de acción de tamaño 6 y un espacio de estado de tamaño 500. Como verá, nuestro algoritmo RL no necesitará más información que estas dos cosas. Todo lo que necesitamos es una forma de identificar un estado de manera única asignando un número único a cada estado posible, y RL aprende a elegir un número de acción del 0 al 5 donde:\n",
        "\n",
        "- 0 = sur\n",
        "- 1 = norte\n",
        "- 2 = este\n",
        "- 3 = oeste\n",
        "- 4 = recoger\n",
        "- 5 = dejar\n",
        "\n",
        "Recuerde que los 500 estados corresponden a una codificación de la ubicación del taxi, la ubicación del pasajero y la ubicación de destino.\n",
        "\n",
        "El aprendizaje por refuerzo aprenderá un mapeo de estados a la acción óptima para realizar en ese estado por exploración , es decir, el agente explora el entorno y realiza acciones en función de las recompensas definidas en el entorno.\n",
        "\n",
        "La acción óptima para cada estado es la acción que tiene la mayor recompensa acumulada a largo plazo."
      ],
      "metadata": {
        "id": "JAtZqjkfqxyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Volver a nuestra ilustración\n",
        "\n",
        "De hecho, podemos tomar nuestra ilustración anterior, codificar su estado y dárselo al entorno para que se renderice en Gym. Recuerde que tenemos el taxi en la fila 3, columna 1, nuestro pasajero está en la ubicación 2 y nuestro destino es la ubicación 0. Usando el método de codificación de estado Taxi-v2, podemos hacer lo siguiente:"
      ],
      "metadata": {
        "id": "mC_3dSERrHNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zqlS_fkYRC2",
        "outputId": "4e6f0b8f-f968-4285-ba44-49a4ad26af6d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estamos usando las coordenadas de nuestra ilustración para generar un número correspondiente a un estado entre 0 y 499, que resulta ser 328 para el estado de nuestra ilustración.\n",
        "\n",
        "Luego, podemos configurar el estado del entorno manualmente `env.env.s`usando ese número codificado. Puedes jugar con los números y verás que el taxi, el pasajero y el destino se mueven."
      ],
      "metadata": {
        "id": "qIxbxfxYrQlc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Caz0WsEJupqk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}